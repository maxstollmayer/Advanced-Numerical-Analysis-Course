{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3\n",
    "\n",
    "$$\n",
    "\\global\\let\\phi=\\varphi\n",
    "\\global\\let\\eps=\\varepsilon\n",
    "\\gdef\\N{\\mathbb{N}}\n",
    "\\gdef\\F{\\mathbb{F}}\n",
    "\\gdef\\R{\\mathbb{R}}\n",
    "\\gdef\\C{\\mathbb{C}}\n",
    "\\gdef\\L{\\mathcal{L}}\n",
    "\\gdef\\rank{\\mathrm{rank}\\,}\n",
    "\\gdef\\mat{\\mathrm{mat}}\n",
    "\\gdef\\norm#1{\\|#1\\|}\n",
    "\\gdef\\matrix#1{\\begin{pmatrix}#1\\end{pmatrix}}\n",
    "\\gdef\\spec{\\bm{\\lambda}}\n",
    "\\gdef\\diag{\\mathrm{diag}}\n",
    "\\gdef\\offdiag{\\mathrm{offdiag}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 3.1\n",
    "\n",
    "Let $n \\in \\N$ and $A \\in \\C^{n \\times n}$. By Theorem 2.21 $A$ has a Schur form $T$. It is easy to check that $A$ is normal, i.e. $AA^* = A^*A$, if and only if $T$ is normal. The latter due to the fact that $T$ being triangular is equivalent to $T$ being diagonal. So the Schur form is a generalization of a diagonal form from normal to general matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral radius and the behavior of matrix powers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition 3.2\n",
    "\n",
    "For $n \\in \\N$ and $A \\in F^{n \\times n}$ the set of all eigenvalues of $A$ is called the **spectrum** of $A$ and is denoted by $\\spec(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition 3.3\n",
    "\n",
    "For $n \\in \\N$ and $A \\in \\F^{n \\times n}$ $\\rho(A) = \\max_{\\lambda \\in \\spec(A)} |\\lambda|$ is called the **spectral radius** of $A$.\n",
    "\n",
    "Any $\\lambda \\in \\spec(A)$ such that $|\\lambda| = \\rho(A)$ is called a **dominant eigenvalue** of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemma 3.4\n",
    "\n",
    "For $n \\in \\N$ let $\\norm{\\cdot}$ be a consistent norm on $\\F^{n \\times n}$. Then $\\rho(A) \\le \\norm{A}$ for any $A \\in \\F^{n \\times n}$.\n",
    "\n",
    "##### Proof\n",
    "\n",
    "Consider any non-zero $y \\in \\F^n$. Let $\\norm{\\cdot}_* : \\F^n \\to \\R,\\ \\norm{x}_* = \\norm{x y^*}$. Since $\\norm{\\cdot}$ is a consistent norm we have\n",
    "$$ \\norm{A x}_* = \\norm{A x y^*} \\le \\norm{A} \\ \\norm{x y^*} = \\norm{A} \\ \\norm{x}_* $$\n",
    "i.e. the norm $\\norm{\\cdot}_*$ is also consistent.\n",
    "\n",
    "Let $\\lambda \\in \\F$ be an eigenvalue of $A$ and $x \\in \\F^n$ be an eigenvector corresponding to that eigenvalue such that $\\norm{x}_* = 1$. Then\n",
    "$$ \\norm{A} = \\norm{A} \\ \\norm{x}_1 \\ge \\norm{A x}_* = \\norm{\\lambda x}_* = |\\lambda| \\ \\norm{x}_* = |\\lambda| $$\n",
    "\n",
    "So we have that $\\rho(A) \\le \\norm{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemma 3.5\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\C^{n \\times n}$ and $\\eps > 0$. Then there exists a consistent norm $\\norm{\\cdot}_{A,\\eps}$ on $\\C^{n \\times n}$ such that $\\norm{A}_{A,\\eps} \\le \\rho(A) + \\eps$.\n",
    "\n",
    "> isn't this trivial for any norm given lemma 3.4? maybe what is meant is that $\\norm{A}_{A,\\eps} \\ge \\rho(A) - \\eps$\n",
    "\n",
    "If the dominant eigenvalues of $A$ are non-defective then there is a consistent norm $\\norm{\\cdot}_A$ on $\\C^{n \\times n}$ such that $\\norm{A}_A = \\rho(A)$.\n",
    "\n",
    "##### Proof\n",
    "\n",
    "By theorem 2.21 matrix $A$ has a Schur decomposition $A = Q T Q^*$ with a unitary matrix $Q \\in \\C^{n \\times n}$ and an upper-triangular matrix $T \\in \\C^{n \\times n}$.\n",
    "\n",
    "Let $\\Lambda = \\diag(T) = \\diag(\\lambda_1, \\dots, \\lambda_n)$ and $U = T - \\Lambda = \\offdiag(T)$ is strictly upper-triangular, meaning $U_{ij} = 0$ for all $j \\in \\{1, \\dots, n\\}$ and $i \\in \\{j, \\dots, n\\}$.\n",
    "\n",
    "For $\\eta > 0$ consider $D_\\eta = \\diag(\\eta^0, \\dots, \\eta^{n-1}) \\in \\C^{n \\times n}$. Then for all $i, j \\in \\{1, \\dots, n\\}$ we have that $(D_\\eta^{-1} U D_\\eta)_{ij} = 0$ if $i \\ge j$ and $(D_\\eta^{-1} U D_\\eta)_{ij} = \\eta^{j-i} U_{ij} \\to 0$ as $\\eta \\to 0$ if $i < j$.\n",
    "\n",
    "Then there exists an $\\eta_* > 0$ such that $\\norm{D_{\\eta_*}^{-1} U D_{\\eta_*}}_\\infty < \\eps$. With $D = D_{\\eta_*}$ we have\n",
    "$$\\begin{align*}\n",
    "    \\norm{D^{-1} Q^* A Q D}_\\infty &= \\norm{D^{-1} \\Lambda D + D^{-1} U D}_\\infty = \\norm{\\Lambda + D^{-1} U D}_\\infty \\\\\n",
    "    &\\le \\norm{\\Lambda}_\\infty + \\norm{D^{-1} U D}_\\infty = \\rho(A) + \\norm{D^{-1} U D}_\\infty \\\\\n",
    "    &< \\rho(A) + \\eps\n",
    "\\end{align*}$$\n",
    "\n",
    "Let us define $\\norm{\\cdot}_{A,\\eps} : \\C^{n \\times n} \\to \\R$ by setting $\\norm{B}_{A,\\eps} = \\norm{D^{-1} Q^* B Q D}_\\infty$ for all $B \\in \\C^{n \\times n}$. By Proposition 2.13 $\\norm{\\cdot}_{A,\\eps}$ is a consistent norm on $\\C^{n \\times n}$. On the other hand we showed above that $\\norm{\\cdot}_{A,\\eps}$ fulfills the claim.\n",
    "\n",
    "Let us now assume that the dominant eigenvalues of $A$ are non-defective.\n",
    "\n",
    "First if $\\rho(A) = 0$ then $\\lambda_1 = \\dots = \\lambda_n = 0$ is a non-defective eigenvalue of $A$ of multiplicity $n$ which means that $A = 0$. Then any consistent norm on $\\C^{n \\times n}$ fulfills the claim.\n",
    "\n",
    "For the remainder of the proof we assume that $\\rho(A) > 0$. Without loss of generality let us also assume that for $k \\in \\{1, \\dots, n \\}$ $|\\lambda_i| = \\rho(A)$ for $i \\in \\{1, \\dots, k \\}$ and $|\\lambda_i| < \\rho(A)$ for $i \\in \\{k+1, \\dots, n \\}$. Since by theorem 2.21 $A$ has a Schur decomposition for any ordering of the eigenvalues, so the dominant ones may be placed at the beginning of the list.\n",
    "\n",
    "If $k = n$ happens to be the case then all eigenvalues of $A$ are non-defective and $A$ is therefore diagonaizable: There exists a non-singular matrix $S \\in \\C^{n \\times n}$ such that $A = S \\Lambda S^{-1}$ where $\\Lambda = \\diag(\\lambda_1, \\dots, \\lambda_n)$. Then $\\norm{\\cdot}_A : \\C^{n \\times n} \\to \\R$ is given by $\\norm{B}_A = \\norm{S^{-1} B S}_\\infty$ for any $B \\in \\C^{n \\times n}$. By proposition 2.13 it is consistent and satisfies $\\norm{A}_A = \\norm{\\Lambda}_A = \\rho(A)$.\n",
    "\n",
    "For the remainder of the proof we assume that $k < n$. Let us consider $\\Lambda_1 = \\diag(\\lambda_1, \\dots, \\lambda_k) \\in \\C^{n \\times n}$ and $\\Lambda_2 = \\diag(\\lambda_{k+1}, \\dots, \\lambda_n) \\in \\C^{n \\times n}$. Then $\\Lambda = \\matrix{\\Lambda_1 && \\\\ && \\Lambda_2}$. Let us also consider the corresponding partitioning of $T = \\matrix{T_1 && T_{12} \\\\ && T_2}$ for $T_1 \\in \\C^{n \\times k}$, $T_{12} \\in \\C^{k \\times (n-k)}$ and $T_2 \\in \\C^{(n-k) \\times (n-k)}$.\n",
    "\n",
    "First we note that $\\chi_A(\\lambda) = \\chi_T(\\lambda) = \\chi_{T_1}(\\lambda) * \\chi_{T_2}(\\lambda)$ for all $\\lambda \\in \\C$. So every dominant eigenvalue $\\lambda \\in \\{\\lambda_1, \\dots, \\lambda_k\\}$ of $A$ is not an eigenvalue of $T_2$ but is an eigenvalue of $T_1$ with the same algebraic multiplicity as for $A$.\n",
    "\n",
    "On the other hand for any $\\lambda \\in \\{\\lambda_1, \\dots, \\lambda_k\\}$ the matrix $T_2 - \\lambda I_2$ is invertible because of the aforementioned reason. So $\\dim \\ker(A - \\lambda I) = \\dim \\ker(T - \\lambda I) = \\dim \\ker(T_1 - \\lambda I_1)$. This implies that every dominant eigenvalue $\\lambda \\in \\{\\lambda_1, \\dots, \\lambda_k\\}$ of $A$ is an eigenvalue for $T_1$ with the same geometric multiplicity.\n",
    "\n",
    "So all eigenvalues of $T_1$ are non-defective which implies that $T_1$ is diagonizable: there exists an invertible matrix $S_1 \\in \\C^{n \\times n}$ such that $S_1^{-1} T_1 S_1 = \\Lambda_1$.\n",
    "\n",
    "Let us consider $S = \\matrix{S_1 && \\\\ && S_2}$. We have $S^{-1} Q^* A Q S = S^{-1} T S = \\matrix{\\Lambda_1 && \\\\ && \\Lambda_2} + \\matrix{0 && T_{12} \\\\ 0 && U_2}$ where $U_2 = T_2 - \\Lambda_2 = \\offdiag(T_2)$.\n",
    "\n",
    "Similarly to how we did the first part of the proof we consider $D = \\diag(\\eta^0, \\dots, \\eta^{n-1}) = \\matrix{D_1 && \\\\ && D_2} \\in \\C^{n \\times n}$ with diagonal blocks $D_1 \\in \\C^{k \\times k}$ and $D_2 \\in \\C^{(n-k) \\times (n-k)}$ for $\\eta > 0$.\n",
    "\n",
    "Then $D^{-1} S^{-1} Q^* A Q S D = \\matrix{\\Lambda_1 && \\\\ && \\Lambda_2} + \\matrix{0 && Z_{12} \\\\ 0 && Z_2}$ where $Z_{12} = D_1^{-1} T_{12} D_2 \\in \\C^{k \\times (n-k)}$ and $Z_2 = D_2^{-1} U_2 D_2$.\n",
    "\n",
    "Block $Z_2$: Since $U_2$ is strictly upper-triangular so is $Z_2$. Furthermore for all $i, j \\in \\{1, \\dots, n-k\\}$ such that $i < j$ we have $(Z_2)_{ij} = \\eta^{j-i} (U_2)_{ij} \\to 0$ as $\\eta \\to 0$.\n",
    "\n",
    "Block $Z_{12}$: For all $i \\in \\{1, \\dots, k\\}$ and $j \\in \\{1, \\dots, n-k\\}$ we have $(Z_{12})_{ij} = \\frac{\\eta^{k+j}}{\\eta^i} (T_{12})_{ij} = \\eta^{k-i} \\eta^j (T_{12})_{ij} \\to 0$ as $\\eta \\to 0$.\n",
    "\n",
    "This implies that $\\bigg\\| \\matrix{Z_{12} \\\\ Z_2} \\bigg\\|_1 \\to 0$ as $\\eta \\to 0$ and then there exists an $\\eta > 0$ such that $\\bigg\\| \\matrix{Z_{12} \\\\ Z_2} \\bigg\\|_1 < \\frac{1}{2} \\big( \\norm{\\Lambda_1}_1 - \\norm{\\Lambda_2}_1 \\big)$. We fix such a value of $\\eta$ for the remainder of the proof.\n",
    "\n",
    "By proposition 2.10 we now have\n",
    "$$\\begin{align*}\n",
    "\\norm{D^{-1} S^{-1} Q^* A Q S D}_1 &= \\max \\bigg\\{ \\norm{\\Lambda_1}_1 , \\bigg\\| \\matrix{Z_{12} \\\\ \\Lambda_2 + Z_2} \\bigg\\|_1 \\bigg\\} \\\\\n",
    "&\\le \\max \\bigg\\{ \\norm{\\Lambda_1}_1 , \\norm{\\Lambda_2}_1 + \\bigg\\| \\matrix{Z_{12} \\\\ Z_2} \\bigg\\|_1 \\bigg\\} \\\\\n",
    "&\\le \\max \\big\\{ \\norm{\\Lambda_1}_1 , \\norm{\\Lambda_2}_1 + {\\textstyle \\frac{1}{2}} \\big( \\norm{\\Lambda_1}_1 - \\norm{\\Lambda_2}_1 \\big) \\big\\} \\\\\n",
    "&= \\norm{\\Lambda_1}_1 = \\rho(A)\n",
    "\\end{align*}$$\n",
    "\n",
    "Let us define $\\norm{\\cdot}_A : \\C^{n \\times n} \\to \\R$ by $\\norm{B}_A = \\norm{D^{-1} S^{-1} Q^* B Q S D}_1$ for any $B \\in \\C^{n \\times n}$. By proposition 2.13 $\\norm{\\cdot}_A$ is a consistent norm on $\\C^{n \\times n}$ and we showed that $\\norm{A}_A = \\norm{\\Lambda_1}_1 = \\rho(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lemma 3.5 which we have just proved will serve us well in the analysis of the behavior of matrix powers in terms of the spectral radius. Here is one result of such type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemma 3.6\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\C^{n \\times n}$, $\\norm{\\cdot}$ be a norm on $\\C^{n \\times n}$ and $\\eps > 0$. Then\n",
    "\n",
    "- there exists constants $c > 0$, depending on $n$ and $\\norm{\\cdot}$ but not on $A$ or $\\eps$, and $C > 0$, depending on $n$, $\\norm{\\cdot}$, $A$ and $\\eps$, such that $c \\rho^k \\le \\norm{A^k} \\le C (\\rho + \\eps)^k$ holds for each $k \\in \\N$ where $\\rho = \\rho(A)$ and\n",
    "\n",
    "- if the dominant eigenvalues of $A$ are non-defective the claim holds for $\\eps = 0$.\n",
    "\n",
    "##### Proof\n",
    "\n",
    "First we establish the lower bound. To this end we consider a dominant eigenvalue $\\lambda \\in \\C$ of $A$ and a corresponding eigenvector $x \\in \\C^n$ of $A$ normalized so that $\\norm{x}_2 = 1$. Then $\\norm{A^kx}_2 = \\norm{\\lambda^k x}_2 = |\\lambda^k| \\norm{x}_2 = \\rho^k$ for any $k \\in \\N$ and hence $\\norm{A^k}_2 \\ge \\rho^k$.\n",
    "\n",
    "By the equivalence of norms there exists a $c > 0$ such that $c \\norm{B}_2 \\le \\norm{B}$ for all $B \\in \\C^{n \\times n}$. This gives in particular $\\norm{A^k} \\ge c \\norm{A^k}_2 \\ge c \\rho^k$ for every $k \\in \\N$.\n",
    "\n",
    "Second we prove the upper bound. By lemma 3.5 there exists a consistent norm $\\norm{\\cdot}_{A,\\eps}$ on $\\C^{n \\times n}$ such that $\\norm{A}_{A,\\eps} \\le \\rho + \\eps$. The consistency yields $\\norm{A^k}_{A,\\eps} \\le \\norm{A}_{A,\\eps}^k \\le (\\rho + \\eps)^k$ for each $k \\in \\N$.\n",
    "\n",
    "By the equivalence of norms there exists a $C > 0$, which depends on $\\norm{\\cdot}_{A,\\eps}$ and hence on $A$ and $\\eps$, such that $\\norm{B} \\le C \\norm{B}_{A,\\eps}$ for all $B \\in \\C^{n \\times n}$. This leads to $\\norm{A^k} \\le C \\norm{A^k}_{A,\\eps} \\le C (\\rho + \\eps)^k$ for each $k \\in \\N$.\n",
    "\n",
    "Finally we note that when the dominant eigenvalues of $A$ are non-defective, lemma 3.5 can be invoked with $\\eps = 0$ and the proof of the upper bound carries over to this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The above lemma allows to bound matrix powers in terms of the spectral radius. Now we will state two basic results that give a bound for the latter. First however, we need a definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition 3.7\n",
    "\n",
    "Let $n \\in \\N$. A matrix $A \\in \\C^{n \\times n}$ is called\n",
    "\n",
    "- **row-wise strictly (non-strictily) diagonally dominant** if $\\displaystyle |A_{ii}| > (\\ge) \\sum_{j \\in \\{1, \\dots, n\\} \\setminus \\{i\\} } |A_{ij}|$ for all $i \\in \\{1, \\dots, n\\}$\n",
    "\n",
    "- **column-wise strictly (non-strictly) diagonally dominant** if $\\displaystyle |A_{jj}| > (\\ge) \\sum_{i \\in \\{1, \\dots, n\\} \\setminus \\{j\\} } |A_{ij}|$ for all $j \\in \\{1, \\dots, n\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem 3.8 (Levy-Desplanaques)\n",
    "\n",
    "Let $n \\in \\N$ and $A \\in \\F^{n \\times n}$ be row-wise or column-wise strictly diagonally dominant. Then $A$ is invertible.\n",
    "\n",
    "##### Proof\n",
    "\n",
    "Consider $A \\in \\F^{n \\times n}$ singular. Then $\\ker A \\ne \\{0\\}$ and there exists $x \\in \\F^n$ such that $A x = 0$. Without loss of generality we shall assume $\\norm{x}_\\infty = 1$. Consider $k \\in \\{1, \\dots, n\\}$ such that $|x_k| = \\norm{x}_\\infty = 1$.\n",
    "\n",
    "Since $A x = 0$ we have in particular $(Ax)_k = 0$, i.e. $A_{kk} x_k = -\\sum_{j \\in \\{1, \\dots, n\\} \\setminus \\{k\\}} A_{kj} x_j$. Then\n",
    "$$\\begin{align*}\n",
    "|A_{kk}| = |A_{kk} x_k| &\\le \\sum_{j \\in \\{1, \\dots, n\\} \\setminus \\{k\\}} |A_{kj} x_j| \\\\\n",
    "&\\le \\sum_{j \\in \\{1, \\dots, n\\} \\setminus \\{k\\}} |A_{kj}|\n",
    "\\end{align*}$$\n",
    "which shows that $A$ is not row-wise strictly diagonally dominant. $A^\\top$ is also singular and, by the same proof, is not row-wise strictly diagonally dominant so that $A$ is not column-wise strictly diagonally dominant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition 3.9 (Gershgorin discs)\n",
    "\n",
    "Let $n \\in \\N$ and $A \\in \\C^{n \\times n}$. For each $k \\in \\{1, \\dots, k\\}$\n",
    "$$ R_k(A) = \\bigg\\{ z \\in \\C \\ : \\ |z - A_{kk}| \\le \\sum_{j \\in \\{1, \\dots, n\\} \\setminus \\{k\\}} |A_{kj}| \\bigg\\} \\subset \\C $$\n",
    "and\n",
    "$$ C_k(A) = \\bigg\\{ z \\in \\C \\ : \\ |z - A_{kk}| \\le \\sum_{i \\in \\{1, \\dots, n\\} \\setminus \\{k\\}} |A_{ik}| \\bigg\\} \\subset \\C $$\n",
    "are called the $k$-th row-wise and column-wise **Gershgorin discs** for $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem 3.10 (First Gershgorin Theorem)\n",
    "\n",
    "Let $n \\in \\N$ and $A \\in \\C^{n \\times n}$.\n",
    "\n",
    "Then $\\displaystyle \\spec(A) \\subseteq \\bigcup_{k=1}^n R_k(A)$ and $\\displaystyle \\spec(A) \\subseteq \\bigcup_{k=1}^n C_k(A)$.\n",
    "\n",
    "##### Proof\n",
    "\n",
    "Consider $A\\in \\C^{n \\times n}$. Let $r_i = \\sum_{j \\in \\{1, \\dots, n\\} \\setminus \\{i\\}} |A_{ij}|$ for each $i \\in \\{1, \\dots, n\\}$. Consider $\\lambda \\in \\C$. If $\\lambda \\in R_i(A)$ for all $i \\in \\{1, \\dots, n\\}$ then $|A_{ii} - \\lambda| > r_i$ for all $i \\in \\{1, \\dots, n\\}$. This however means that the matrix $A - \\lambda I$ is row-wise strictly diagonally dominant. By theorem 3.8 it is therefore non-singular, which means that $\\lambda$ is not an eigenvalue of $A$.\n",
    "\n",
    "We have shown $\\lambda \\notin \\bigcup_{i=1}^n R_i(A) \\implies \\lambda \\notin \\spec(A)$, which yields $\\spec(A) \\subseteq \\bigcup_{i=1}^n R_i(A)$.\n",
    "\n",
    "Applying this result to $A^\\top$ in place of $A$, we obtain $\\spec(A) = \\spec(A^\\top) \\subseteq \\bigcup_{i=1}^n R_i(A^\\top) = \\bigcup_{i=1}^n C_i(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative methods for linear systems\n",
    "\n",
    "In this part of the course we consider the problem of finding $x \\in \\F^n$ such that $Ax=b$ where $n \\in \\N$ and the data $A \\in \\F^{n \\times n}$ and $b \\in \\F^n$ are given.\n",
    "\n",
    "In contrast to the approach of direct methods, iterative methods do not aim at computing the solution with the best precision possible at once. Instead they consist in the sequential construction of iterates $x_k \\in \\F^n$, $k \\in \\N$, starting from some given initial guess $x_0 \\in \\F^n$, which approximate the exact solution $x$ ever better as $k$ increases, assuming that the problem has a unique solution.\n",
    "\n",
    "For many iterative methods, one has the relation $e_k = T e_{k-1} = \\dots = T^k e_0$ for each $k \\in \\N$, where $e_k = x_k - x$ is the error of the $k$-th iterate for each $k \\in \\N_0$ and $T$ is the iteration matrix.\n",
    "\n",
    "We start with two classical iterative methods: the Jacobi and Gauß-Seidel methods. Both are based on a splitting $A = A_1 + A_2$ that induces the corresponding iterative process $A_1 x_k + A_2 x_{k-1} = b$ for $k \\in \\N$.\n",
    "\n",
    "Consider $A \\in \\F^{n \\times n}$ with $n \\in \\N$ and let $D,L,U \\in \\F^{n \\times n}$ denote the diagonal, strictly lower-triangular and strictly upper-triangular parts of $A$. Then $A = D + L + U$.\n",
    "\n",
    "The **Jacobi iteration** is $D x_k = -(L + U) x_{n-1} + b$.\n",
    "\n",
    "The **Gauß-Seidel iteration** reads $(D + L) x_k = - U x_{k-1} + b$.\n",
    "\n",
    "The errors of these methods evolve as follows:\n",
    "\n",
    "$D e_k = D x_k - D x = -(L + U) x_{k-1} + b - A x + (L + U) x = -(L + U) e_{k-1}$ for all $k \\in \\N$ for the Jacobi iteration and $(D + L) e_k = (D + L) x_k - (D + L) x = -U x_{k-1} + b - A x + U x = -U e_{k-1}$ for all $k \\in \\N$ for the Gauß-Seidel iteration.\n",
    "\n",
    "These methods require that none of the diagonal elements of $A$ are zero, in which case we have:\n",
    "\n",
    "$x_k = D^{-1} (b - (L + U) x_{k-1})$ and $e_k = T e_{k-1}$ with $T = -D^{-1} (L + U)$ for the Jacobi iteration.\n",
    "\n",
    "$x_k = (D + L)^{-1} (b - U x_{k-1})$ and $e_k = T e_{k-1}$ with $T = -(D + L)^{-1} U$ for the Gauß-Seidel iteration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
