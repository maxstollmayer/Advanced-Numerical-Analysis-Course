{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5\n",
    "\n",
    "$$\n",
    "\\global\\let\\phi=\\varphi\n",
    "\\global\\let\\eps=\\varepsilon\n",
    "\\gdef\\N{\\mathbb{N}}\n",
    "\\gdef\\F{\\mathbb{F}}\n",
    "\\gdef\\R{\\mathbb{R}}\n",
    "\\gdef\\C{\\mathbb{C}}\n",
    "\\gdef\\L{\\mathcal{L}}\n",
    "\\gdef\\rank{\\mathrm{rank}\\,}\n",
    "\\gdef\\mat{\\mathrm{mat}}\n",
    "\\gdef\\norm#1{\\|#1\\|}\n",
    "\\gdef\\matrix#1{\\begin{pmatrix}#1\\end{pmatrix}}\n",
    "\\gdef\\spec{\\bm{\\lambda}}\n",
    "\\gdef\\diag{\\mathrm{diag}}\n",
    "\\gdef\\offdiag{\\mathrm{offdiag}}\n",
    "\\gdef\\O{\\mathcal{O}}\n",
    "\\gdef\\nto{\\nrightarrow}\n",
    "\\gdef\\Re{\\mathrm{Re}}\n",
    "\\gdef\\Im{\\mathrm{Im}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue considering a linear-iterative method $x_k = x_{k-1} + P_k^{-1} r_{k-1}$ for $k \\in \\N$ for the linear system $A x = b$ where $A \\in \\F^{n \\times n}$, $b \\in \\F^n$ and $r_k = b - A x_k$. When $P_k$ with $k \\in \\N$ are are identical the subscript index may be omitted; the iterative method then takes the form $x_k = x_{k-1} + P^{-1} r_{k-1}$ and is called **stationary**. The Jacobi and Gauß-Seidel methods are stationary linear iterative methods corresponding to $P = D$ and $P = D + L$ respectively.\n",
    "\n",
    "The design of a stationary linear iterative method aims at the following:\n",
    "- $u \\mapsto P^{-1} u should be easy to evaluate,\n",
    "- $P$ should approximate $A$ in the sense that $P^{-1} A \\approx A$, precisely, $\\rho(B)$ for $B = I - P^{-1} A$ should be as small as possible.\n",
    "So $P$ should be a surrogate for $A$ with which linear systems can easily be solved. $P$ is often called a **preconditioner** for $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 5.1\n",
    "\n",
    "- Jacobi: $P = D$ is diagonal, $P^{-1}$ is applied in $\\O(n)$ operations.\n",
    "- Gauß-Seidel: $P = D + L$ is lower-triangular, $P^{-1}$ is applied in $\\O(n^2)$ iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 5.2\n",
    "\n",
    "Related stationary linear iterative methods:\n",
    "\n",
    "- **backward Gauß-Seidel** method corresponds to $P = D + U$. The behaviour and convergence analysis are completeley analogous to those of the Gauß-Seidel method.\n",
    "- **Jacobi over-relaxation** (JOR) mehtod corresponds to $P = \\frac{1}{\\omega} D$, where $D = \\diag(A)$ and $\\omega \\ne 0$ is a **relaxation parameter**. The iteration takes the form $x_k = x_{k-1} + \\omega D^{-1} r_{k-1}$ for all $k \\in \\N$, so the closer $\\omega$ is to 0, the less a single iteration \"learns\" from the current residual (in machine learning, \"learning rate\" is a more standard term than \"relaxation parameter\"). For any invertible matrix $A \\in \\C^{n \\times n}$ such that $\\rho(D^{-1}A) < 1$, where $D = diag(A)$ and for any $b \\in \\C^n$ and $x_0 \\in \\C^n$, the Jacobi method with initial guess $x_0$ converges (see analysis above), and the JOR method with any $\\omega \\in (0, 1)$ and initial guess $x_0 \\in \\C^n$ converges too but slower. When $A \\in \\C^{n \\times n}$ is symmetric positive-definite, the JOR method for $A x = b$ with $0 < \\omega < \\frac{2}{\\rho(D^{-1} A)}$ converges for any initial guess $x_0$.\n",
    "- **successive over-relaxation** corresponds to $P = \\frac{1}{\\omega} D + L$, where again $\\omega \\ne 0$ is a relaxation parameter. The particular case of $\\omega = 1$ is the Gauß-Seidel method. The iteration takes the form $x_k = x_{k-1} + \\omega(D + \\omega L)^{-1} r_{k-1}$ for all $k \\in \\N$, so the role of $\\omega$ is more sophisticated than switching to what extent the iteration follows the Gauß-Seidel method and to what extent, does nothing and retains the current iterate. The method doesn't converge for $\\omega \\notin (0,2)$. When matrix $A$ is symmetric positive-definite, the method converges for $\\omega \\in (0,2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 5.3\n",
    "\n",
    "Note that the behaviour of a linear iterative method corresponding to matrices $P_k$ with $k \\in \\N$, once an invertible system matrix has been fixed is completely determined by any of\n",
    "- the initial residual,\n",
    "- the initial error.\n",
    "Indeed, for any right-hand sight $b \\in \\F^n$ and any initial guess $x_0 \\in \\F^n$, we have for all $k \\in \\N$\n",
    "$$\\begin{align*}\n",
    "x_k &= x_{k-1} + P_k^{-1} r_{k-1}, \\\\\n",
    "r_k &= b - A x_k = r_{k-1} - A P_k^{-1} r_{k-1} = (I - A P_k^{-1}) r_{k-1}, \\\\\n",
    "e_k &= A^{-1} r_k = (I - P_k^{-1} A) e_{k-1} = B_k e_{k-1}.\n",
    "\\end{align*}$$\n",
    "\n",
    "So each of $r_0 = b - A x_0$ and $e_0 = x - x_0$ uniquely defines all $x_k$ with $k \\in \\N$.\n",
    "\n",
    "On the other hand, we can always assume $x_0 = 0$ since replacing $b \\in \\F^n$ and $x_0 \\in \\F^n$ with $b - A x_0$ and 0 affects neither the initial error nor the initial residual and therefore doesn't affect the behaviour of the method. These are all, of course, consequences of the lineartiy of the problem and of the iterative method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem 5.4\n",
    "\n",
    "Let $n \\in \\N$ and consider an invertible matrix $A \\in \\C^{n \\times n}$. A stationary linear iterative method corresponding to an invertible matrix $P$ and zero initial guess converges for $A x = b$ with any $b \\in \\C^n$ if and only if $\\rho(I - P^{-1} A) < 1$.\n",
    "\n",
    "##### Proof\n",
    "\n",
    "Let $\\rho = \\rho(I - P^{-1} A)$. We choose a norm $\\norm{\\cdot}$ on $\\C^n$ and the corresponding operator norm $\\norm{\\cdot}$ on $\\C^{n \\times n}$, so that $\\norm{B^k u} \\le \\norm{B^k} \\norm{u}$ for any $u \\in \\C^n$.\n",
    "\n",
    "- If $\\rho < 1$, the upper bound of lemma 3.6 with $\\eps = \\frac{1}{2}(1 - \\rho)$ yields the convergence of the iterative method for any right-hand side $b \\in C^n$.\n",
    "- If $\\rho \\ge 1$, consider an eigenvector $u \\in \\C^n$ corresponding to a dominant eigenvalue of $B$. Then $\\norm{B^k u} = \\rho^k \\norm{u}$ so that $B^k u \\nto 0$ as $k \\to \\infty$, i.e., the method does not converge when $e_0 = u$ (for $b = Au$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 5.5\n",
    "\n",
    "In the case of stationary linear iterative methods, we can first \"precondition\" the linear system, replacing $A$ and $b$ with $\\tilde{A} = P^{-1} A$ and $\\tilde{b} = P^{-1} b$. It's not just that the systems $A x = b$ and $\\tilde{A} x = \\tilde{b}$ are equivalent but also the iterates generated by the method $Q$ in place of $P$ for $\\tilde{A} x = \\tilde{b}$ coincide with the iterates of the original method for $A x = b$:\n",
    "$ \\tilde{x}_k = \\tilde{x}_{k-1} + \\tilde{r}_{k-1}$ with $\\tilde{r}_{k-1} = P^{-1} b - P^{-1} A \\tilde{x}_{k-1}$ for all $k \\in \\N$, which gives $\\tilde{r}_{k-1} = P^{-1} r_{k-1}$ and $\\tilde{x}_k = x_k$ if $\\tilde{x}_0 = x_0$.\n",
    "\n",
    "This may be not very practical because computing $P^{-1} A$ may be much more expensive than computing $P^{-1} r_{k-1}$ with a suitable range of $k$, but is convenient for analysis. Namely, we can assume that $P = Q$ and $\\rho(Q - A) < 1$ for matrix $A$ preconditioned beforehand.\n",
    "\n",
    "Alternatively we may leave some simple part of preconditioning in the iteration and may even let that simple preconditioning vary from one iteration to another, i.e. a non-stationary method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition 5.6\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\F^{n \\times n}$ and $b \\in \\F^n$. The **stationary Richardson method** for $A x = b$ starts from an initial guess $x_0 \\in \\F^n$ and reads $x_k = x_{k-1} + \\alpha r_{k-1}$ for all $k \\in \\N$, where $\\alpha \\in \\R \\setminus \\{0\\}$ is the relaxation parameter or so-called **acceleration parameter**.\n",
    "\n",
    "The **non-stationary Richardson method** for $A x = b$ starts from an initial guess $x_0 \\in \\F^n$ and reads $x_k = x_{k-1} + \\alpha_k r_{k-1}$ for all $k \\in \\N$, where $\\alpha_k \\in \\R \\setminus \\{0\\}$ are iteration-dependent relaxation (acceleration) parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the stationary case, as we discussed in remark 5.5 above, one can replace $A$, $b$ and $\\alpha$ with $\\frac{1}{\\alpha} A$, $\\frac{1}{\\alpha} b$ and 1 without modifying the iterative process. It is, however, standard to assume that $\\rho(Q - \\alpha A) < 1$ can be ensured by a suitable choice of $\\alpha$ and keep the relaxation parameter in the iteration. In the non-stationary case, varying the relaxation parameter from iteration to iteration will allow to accelerate the convergence of the method.\n",
    "> \n",
    "> As we will now see, the system matrix $A$ will be required to be well-behaving (procnditioned beforehand if necessary), and $P_k = \\frac{1}{\\alpha_k}Q$ with $k \\in \\N$ are the \"simple preconditioners\" we formally retain in the iterative method to fine-tune it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem 5.7\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\C^{n \\times n}$. The stationary Richardson method with zero initial guess converges for $A x = b$ with any $b \\in \\C^n$ if and only if $\\frac{2 \\Re \\lambda}{\\alpha |\\lambda|^2} > 1$ for all $\\lambda \\in \\spec(A)$.\n",
    "\n",
    "##### Proof\n",
    "\n",
    "The iteration matrix of the method is $B = Q - \\alpha Q$ so that $\\spec(B) = 1 - \\alpha \\spec(A)$ and hence $\\rho(B) = \\max_{\\lambda \\in \\spec(A)} |1 - \\alpha \\lambda|. By theorem 5.4 the methods converges for all $b \\in \\C^n$ if and only if $\\rho(B) < 1$.\n",
    "\n",
    "Since $\\alpha \\in \\R$ we have for any $\\lambda \\in \\C$\n",
    "$$\\begin{align*}\n",
    "& |1 - \\alpha \\lambda|^2 = (\\alpha \\, \\Im \\lambda)^2 + (1 - \\alpha \\, \\Re \\lambda)^2 = \\alpha^2 |\\lambda|^2 + 1 - 2 \\alpha \\, \\Re \\lambda < 1 \\\\\n",
    "& \\iff \\alpha^2 |\\lambda|^2 - 2 \\, \\Re \\lambda < 0 \\\\\n",
    "& \\iff \\frac{2 \\, \\Re \\lambda}{\\alpha |\\lambda|^2} > 1.\n",
    "\\end{align*}$$\n",
    "\n",
    "So $\\rho(B) < 1 \\iff \\dfrac{2 \\, \\Re \\lambda}{\\alpha |\\lambda|^2} > 1$ for all $\\lambda \\in \\spec(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem 5.8\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\C^{n \\times n}$ with $\\spec(A) = \\{\\lambda_1, \\dots, \\lambda_n\\} \\subset \\R$, where $\\lambda_1 \\ge \\dots \\ge \\lambda_n > 0$. Then the stationary Richardson iteration converges for $A x = b$ with any $b \\in \\C^n$ if and only if $\\alpha \\in (0, \\frac{2}{\\lambda_1}).\n",
    "\n",
    "Furthermore $\\alpha_* = \\frac{2}{\\lambda_1 + \\lambda_n}$ is a unique minimizer of $\\rho(B)$ w.r.t. $\\alpha \\in \\R \\setminus \\{0\\}$ and yields $\\rho(B) = \\frac{\\kappa-1}{\\kappa+1}$, where $\\kappa = \\frac{\\lambda_1}{\\lambda_n}$ is the spectral condition number of $A$.\n",
    "\n",
    "##### Proof\n",
    "\n",
    "The convergence criterion follows from theorem 5.7: $\\rho(B) < 1$ holds if and only if $\\alpha > 0$ and $\\alpha \\rho(A) < 2$, i.e., $\\alpha \\in (0, \\frac{2}{\\lambda_1})$.\n",
    "\n",
    "To prove the second claim we consider $\\alpha \\in (0, \\frac{2}{\\lambda_1})$ and $\\phi_\\alpha : \\R \\to \\R$ given by $\\phi_\\alpha(\\lambda) = |1 - \\alpha \\lambda|$ for all $\\lambda \\in \\R$. As we noted in the proof of theorem 5.7, $\\rho(B) = \\max_{\\lambda \\in \\spec(A)} \\phi_\\alpha(\\lambda)$. Since "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
