{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5\n",
    "\n",
    "$$\n",
    "\\global\\let\\phi=\\varphi\n",
    "\\global\\let\\eps=\\varepsilon\n",
    "\\gdef\\N{\\mathbb{N}}\n",
    "\\gdef\\F{\\mathbb{F}}\n",
    "\\gdef\\R{\\mathbb{R}}\n",
    "\\gdef\\C{\\mathbb{C}}\n",
    "\\gdef\\L{\\mathcal{L}}\n",
    "\\gdef\\rank{\\mathrm{rank}\\,}\n",
    "\\gdef\\mat{\\mathrm{mat}}\n",
    "\\gdef\\norm#1{\\|#1\\|}\n",
    "\\gdef\\matrix#1{\\begin{pmatrix}#1\\end{pmatrix}}\n",
    "\\gdef\\spec{\\bm{\\lambda}}\n",
    "\\gdef\\diag{\\mathrm{diag}}\n",
    "\\gdef\\offdiag{\\mathrm{offdiag}}\n",
    "\\gdef\\O{\\mathcal{O}}\n",
    "\\gdef\\nto{\\nrightarrow}\n",
    "\\gdef\\Re{\\mathrm{Re}}\n",
    "\\gdef\\Im{\\mathrm{Im}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue considering a linear-iterative method $x_k = x_{k-1} + P_k^{-1} r_{k-1}$ for $k \\in \\N$ for the linear system $A x = b$ where $A \\in \\F^{n \\times n}$, $b \\in \\F^n$ and $r_k = b - A x_k$. When $P_k$ with $k \\in \\N$ are are identical the subscript index may be omitted; the iterative method then takes the form $x_k = x_{k-1} + P^{-1} r_{k-1}$ and is called **stationary**. The Jacobi and Gauß-Seidel methods are stationary linear iterative methods corresponding to $P = D$ and $P = D + L$ respectively.\n",
    "\n",
    "The design of a stationary linear iterative method aims at the following:\n",
    "- $u \\mapsto P^{-1} u should be easy to evaluate,\n",
    "- $P$ should approximate $A$ in the sense that $P^{-1} A \\approx A$, precisely, $\\rho(B)$ for $B = I - P^{-1} A$ should be as small as possible.\n",
    "So $P$ should be a surrogate for $A$ with which linear systems can easily be solved. $P$ is often called a **preconditioner** for $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 5.1\n",
    "\n",
    "- Jacobi: $P = D$ is diagonal, $P^{-1}$ is applied in $\\O(n)$ operations.\n",
    "- Gauß-Seidel: $P = D + L$ is lower-triangular, $P^{-1}$ is applied in $\\O(n^2)$ iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 5.2\n",
    "\n",
    "Related stationary linear iterative methods:\n",
    "\n",
    "- **backward Gauß-Seidel** method corresponds to $P = D + U$. The behaviour and convergence analysis are completeley analogous to those of the Gauß-Seidel method.\n",
    "- **Jacobi over-relaxation** (JOR) mehtod corresponds to $P = \\frac{1}{\\omega} D$, where $D = \\diag(A)$ and $\\omega \\ne 0$ is a **relaxation parameter**. The iteration takes the form $x_k = x_{k-1} + \\omega D^{-1} r_{k-1}$ for all $k \\in \\N$, so the closer $\\omega$ is to 0, the less a single iteration \"learns\" from the current residual (in machine learning, \"learning rate\" is a more standard term than \"relaxation parameter\"). For any invertible matrix $A \\in \\C^{n \\times n}$ such that $\\rho(D^{-1}A) < 1$, where $D = diag(A)$ and for any $b \\in \\C^n$ and $x_0 \\in \\C^n$, the Jacobi method with initial guess $x_0$ converges (see analysis above), and the JOR method with any $\\omega \\in (0, 1)$ and initial guess $x_0 \\in \\C^n$ converges too but slower. When $A \\in \\C^{n \\times n}$ is symmetric positive-definite, the JOR method for $A x = b$ with $0 < \\omega < \\frac{2}{\\rho(D^{-1} A)}$ converges for any initial guess $x_0$.\n",
    "- **successive over-relaxation** corresponds to $P = \\frac{1}{\\omega} D + L$, where again $\\omega \\ne 0$ is a relaxation parameter. The particular case of $\\omega = 1$ is the Gauß-Seidel method. The iteration takes the form $x_k = x_{k-1} + \\omega(D + \\omega L)^{-1} r_{k-1}$ for all $k \\in \\N$, so the role of $\\omega$ is more sophisticated than switching to what extent the iteration follows the Gauß-Seidel method and to what extent, does nothing and retains the current iterate. The method doesn't converge for $\\omega \\notin (0,2)$. When matrix $A$ is symmetric positive-definite, the method converges for $\\omega \\in (0,2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 5.3\n",
    "\n",
    "Note that the behaviour of a linear iterative method corresponding to matrices $P_k$ with $k \\in \\N$, once an invertible system matrix has been fixed is completely determined by any of\n",
    "- the initial residual,\n",
    "- the initial error.\n",
    "Indeed, for any right-hand sight $b \\in \\F^n$ and any initial guess $x_0 \\in \\F^n$, we have for all $k \\in \\N$\n",
    "$$\\begin{align*}\n",
    "x_k &= x_{k-1} + P_k^{-1} r_{k-1}, \\\\\n",
    "r_k &= b - A x_k = r_{k-1} - A P_k^{-1} r_{k-1} = (I - A P_k^{-1}) r_{k-1}, \\\\\n",
    "e_k &= A^{-1} r_k = (I - P_k^{-1} A) e_{k-1} = B_k e_{k-1}.\n",
    "\\end{align*}$$\n",
    "\n",
    "So each of $r_0 = b - A x_0$ and $e_0 = x - x_0$ uniquely defines all $x_k$ with $k \\in \\N$.\n",
    "\n",
    "On the other hand, we can always assume $x_0 = 0$ since replacing $b \\in \\F^n$ and $x_0 \\in \\F^n$ with $b - A x_0$ and 0 affects neither the initial error nor the initial residual and therefore doesn't affect the behaviour of the method. These are all, of course, consequences of the lineartiy of the problem and of the iterative method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem 5.4\n",
    "\n",
    "Let $n \\in \\N$ and consider an invertible matrix $A \\in \\C^{n \\times n}$. A stationary linear iterative method corresponding to an invertible matrix $P$ and zero initial guess converges for $A x = b$ with any $b \\in \\C^n$ if and only if $\\rho(I - P^{-1} A) < 1$.\n",
    "\n",
    "##### Proof\n",
    "\n",
    "Let $\\rho = \\rho(I - P^{-1} A)$. We choose a norm $\\norm{\\cdot}$ on $\\C^n$ and the corresponding operator norm $\\norm{\\cdot}$ on $\\C^{n \\times n}$, so that $\\norm{B^k u} \\le \\norm{B^k} \\norm{u}$ for any $u \\in \\C^n$.\n",
    "\n",
    "- If $\\rho < 1$, the upper bound of lemma 3.6 with $\\eps = \\frac{1}{2}(1 - \\rho)$ yields the convergence of the iterative method for any right-hand side $b \\in C^n$.\n",
    "- If $\\rho \\ge 1$, consider an eigenvector $u \\in \\C^n$ corresponding to a dominant eigenvalue of $B$. Then $\\norm{B^k u} = \\rho^k \\norm{u}$ so that $B^k u \\nto 0$ as $k \\to \\infty$, i.e., the method does not converge when $e_0 = u$ (for $b = Au$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 5.5\n",
    "\n",
    "In the case of stationary linear iterative methods, we can first \"precondition\" the linear system, replacing $A$ and $b$ with $\\tilde{A} = P^{-1} A$ and $\\tilde{b} = P^{-1} b$. It's not just that the systems $A x = b$ and $\\tilde{A} x = \\tilde{b}$ are equivalent but also the iterates generated by the method $I$ in place of $P$ for $\\tilde{A} x = \\tilde{b}$ coincide with the iterates of the original method for $A x = b$:\n",
    "$ \\tilde{x}_k = \\tilde{x}_{k-1} + \\tilde{r}_{k-1}$ with $\\tilde{r}_{k-1} = P^{-1} b - P^{-1} A \\tilde{x}_{k-1}$ for all $k \\in \\N$, which gives $\\tilde{r}_{k-1} = P^{-1} r_{k-1}$ and $\\tilde{x}_k = x_k$ if $\\tilde{x}_0 = x_0$.\n",
    "\n",
    "This may be not very practical because computing $P^{-1} A$ may be much more expensive than computing $P^{-1} r_{k-1}$ with a suitable range of $k$, but is convenient for analysis. Namely, we can assume that $P = I$ and $\\rho(I - A) < 1$ for matrix $A$ preconditioned beforehand.\n",
    "\n",
    "Alternatively we may leave some simple part of preconditioning in the iteration and may even let that simple preconditioning vary from one iteration to another, i.e. a non-stationary method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition 5.6\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\F^{n \\times n}$ and $b \\in \\F^n$. The **stationary Richardson method** for $A x = b$ starts from an initial guess $x_0 \\in \\F^n$ and reads $x_k = x_{k-1} + \\alpha r_{k-1}$ for all $k \\in \\N$, where $\\alpha \\in \\R \\setminus \\{0\\}$ is the relaxation parameter or so-called **acceleration parameter**.\n",
    "\n",
    "The **non-stationary Richardson method** for $A x = b$ starts from an initial guess $x_0 \\in \\F^n$ and reads $x_k = x_{k-1} + \\alpha_k r_{k-1}$ for all $k \\in \\N$, where $\\alpha_k \\in \\R \\setminus \\{0\\}$ are iteration-dependent relaxation (acceleration) parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the stationary case, as we discussed in remark 5.5 above, one can replace $A$, $b$ and $\\alpha$ with $\\frac{1}{\\alpha} A$, $\\frac{1}{\\alpha} b$ and 1 without modifying the iterative process. It is, however, standard to assume that $\\rho(I - \\alpha A) < 1$ can be ensured by a suitable choice of $\\alpha$ and keep the relaxation parameter in the iteration. In the non-stationary case, varying the relaxation parameter from iteration to iteration will allow to accelerate the convergence of the method.\n",
    "> \n",
    "> As we will now see, the system matrix $A$ will be required to be well-behaving (procnditioned beforehand if necessary), and $P_k = \\frac{1}{\\alpha_k} I$ with $k \\in \\N$ are the \"simple preconditioners\" we formally retain in the iterative method to fine-tune it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem 5.7\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\C^{n \\times n}$. The stationary Richardson method with zero initial guess converges for $A x = b$ with any $b \\in \\C^n$ if and only if $\\frac{2 \\Re \\lambda}{\\alpha |\\lambda|^2} > 1$ for all $\\lambda \\in \\spec(A)$.\n",
    "\n",
    "##### Proof\n",
    "\n",
    "The iteration matrix of the method is $B = I - \\alpha A$ so that $\\spec(B) = 1 - \\alpha \\spec(A)$ and hence $\\rho(B) = \\max_{\\lambda \\in \\spec(A)} |1 - \\alpha \\lambda|. By theorem 5.4 the methods converges for all $b \\in \\C^n$ if and only if $\\rho(B) < 1$.\n",
    "\n",
    "Since $\\alpha \\in \\R$ we have for any $\\lambda \\in \\C$\n",
    "$$\\begin{align*}\n",
    "& |1 - \\alpha \\lambda|^2 = (\\alpha \\, \\Im \\lambda)^2 + (1 - \\alpha \\, \\Re \\lambda)^2 = \\alpha^2 |\\lambda|^2 + 1 - 2 \\alpha \\, \\Re \\lambda < 1 \\\\\n",
    "& \\iff \\alpha^2 |\\lambda|^2 - 2 \\, \\Re \\lambda < 0 \\\\\n",
    "& \\iff \\frac{2 \\, \\Re \\lambda}{\\alpha |\\lambda|^2} > 1.\n",
    "\\end{align*}$$\n",
    "\n",
    "So $\\rho(B) < 1 \\iff \\dfrac{2 \\, \\Re \\lambda}{\\alpha |\\lambda|^2} > 1$ for all $\\lambda \\in \\spec(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem 5.8\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\C^{n \\times n}$ with $\\spec(A) = \\{\\lambda_1, \\dots, \\lambda_n\\} \\subset \\R$, where $\\lambda_1 \\ge \\dots \\ge \\lambda_n > 0$. Then the stationary Richardson iteration converges for $A x = b$ with any $b \\in \\C^n$ if and only if $\\alpha \\in (0, \\frac{2}{\\lambda_1}).\n",
    "\n",
    "Furthermore $\\alpha_* = \\frac{2}{\\lambda_1 + \\lambda_n}$ is a unique minimizer of $\\rho(B)$ w.r.t. $\\alpha \\in \\R \\setminus \\{0\\}$ and yields $\\rho(B) = \\frac{\\kappa-1}{\\kappa+1}$, where $\\kappa = \\frac{\\lambda_1}{\\lambda_n}$ is the spectral condition number of $A$.\n",
    "\n",
    "##### Proof\n",
    "\n",
    "The convergence criterion follows from theorem 5.7: $\\rho(B) < 1$ holds if and only if $\\alpha > 0$ and $\\alpha \\rho(A) < 2$, i.e., $\\alpha \\in (0, \\frac{2}{\\lambda_1})$.\n",
    "\n",
    "To prove the second claim we consider $\\alpha \\in (0, \\frac{2}{\\lambda_1})$ and $\\phi_\\alpha : \\R \\to \\R$ given by $\\phi_\\alpha(\\lambda) = |1 - \\alpha \\lambda|$ for all $\\lambda \\in \\R$. As we noted in the proof of theorem 5.7, $\\rho(B) = \\max_{\\lambda \\in \\spec(A)} \\phi_\\alpha(\\lambda)$. Since $\\phi_\\alpha$ is a convex function and $\\lambda_1 \\ge \\dots \\ge \\lambda_n$, we have $ \\rho(B) = \\max\\{ \\phi_\\alpha(\\lambda_1), \\phi_\\alpha(\\lambda_n) \\}$.\n",
    "\n",
    "First for $\\hat{\\lambda} = \\frac{1}{\\alpha}$, we have $\\phi_\\alpha(\\hat{\\lambda}) = 0$ and $\\phi'_\\alpha(\\lambda) = \\begin{cases} \\alpha &\\quad \\lambda > \\hat{\\lambda} \\\\ -\\alpha &\\quad \\lambda < \\hat{\\lambda} \\end{cases}$ for all $\\lambda \\in \\R$.\n",
    "\n",
    "Note that $\\frac{1}{\\alpha_*} = \\frac{\\lambda_1 + \\lambda_n}{2}$ is the middle of the interval of interest, $[\\lambda_n, \\lambda_1]$.\n",
    "\n",
    "- So if $\\frac{1}{\\alpha} = \\hat{\\lambda} > \\frac{\\lambda_1 + \\lambda_n}{2} = \\frac{1}{\\alpha_*}$ then $\\lambda_1 - \\hat{\\lambda} < \\hat{\\lambda} - \\lambda_n$ and hence $\\phi_\\alpha(\\lambda_1) < \\phi_\\alpha(\\lambda_n)$, and we have\n",
    "$$\\begin{align*}\n",
    "\\rho(B) &= \\phi_\\alpha(\\lambda_n) = |1 - \\alpha \\lambda_n| \\overset{\\lambda_n < \\hat{\\lambda}}{=} 1 - \\alpha \\lambda_n \\\\\n",
    "&\\overset{\\alpha < \\alpha_*}{>} 1 - \\alpha_* \\lambda_n = 1 - \\frac{2 \\lambda_n}{\\lambda_1 + \\lambda_n} = \\frac{\\lambda_1 - \\lambda_n}{\\lambda_1 + \\lambda_n}.\n",
    "\\end{align*}$$\n",
    "\n",
    "- Similarly, if $\\frac{1}{\\alpha} = \\hat{\\lambda} < \\frac{\\lambda_1 + \\lambda_n}{2} = \\frac{1}{\\alpha_*}$, then $\\lambda_1 - \\hat{\\lambda} > \\hat{\\lambda} - \\lambda_n$ and hence $\\phi_\\alpha(\\lambda_1) > \\phi_\\alpha(\\lambda_n)$, so that\n",
    "$$\\begin{align*}\n",
    "\\rho(B) &= \\phi_\\alpha(\\lambda_1) = |1 - \\alpha \\lambda_1| \\overset{\\lambda_1 > \\hat{\\lambda}}{=} \\alpha \\lambda_1 - 1 \\\\\n",
    "&\\overset{\\alpha > \\alpha_*}{>} \\alpha_* \\lambda_1 - 1 = \\frac{2 \\lambda_1}{\\lambda_1 + \\lambda_n} - 1 = \\frac{\\lambda_1 - \\lambda_n}{\\lambda_1 + \\lambda_n}.\n",
    "\\end{align*}$$\n",
    "\n",
    "- On the other hand, if $\\alpha = \\alpha_*$, then $\\lambda_1 - \\hat{\\lambda} = \\hat{\\lambda} - \\lambda_n$, so that $\\phi_\\alpha(\\lambda_1) = \\lambda_\\alpha(\\lambda_n)$ and\n",
    "$$\\begin{align*}\n",
    "\\rho(B) &= \\phi_\\alpha(\\lambda_1) = |1 - \\alpha \\lambda_1| \\\\\n",
    "&\\overset{\\hat{\\lambda} \\le \\lambda_1}{=} \\alpha \\lambda_1 - 1 = \\alpha_* \\lambda_1 - 1 = \\frac{\\lambda_1 - \\lambda_n}{\\lambda_1 + \\lambda_n}.\n",
    "\\end{align*}$$\n",
    "\n",
    "The three cases considered above show that $\\rho(B) = \\frac{\\kappa - 1}{\\kappa + 1}$ if $\\alpha = \\alpha_*$ and $\\rho(B) > \\frac{\\kappa - 1}{\\kappa + 1}$ if $\\alpha \\in \\R \\setminus \\{0, \\alpha_*\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative methods for systems with SPD matrices\n",
    "\n",
    "Theorem 5.8 is applicable, in particular, when matrix $A$ is real, sysmmetric positive definite. In this case, the linear system $A x = b$ with $b \\in \\R^n$ has a very natural interpretation in terms of optimization: it is a necessary and sufficient condition for $x \\in \\R^n$ being an extremum of the functional $J : \\R^n \\to \\R$ given by $J(x) = \\frac{1}{2} x^\\top A x - b^\\top x$. Note that $\\nabla J(x) = A x . b = -r(x)$ and $\\nabla^2 J(x) = A$.\n",
    "\n",
    "This functional is quadratic and, since $\\nabla^2 J = A$ is symmetric and positive definite, also strongly convex, so it has a unique critical point $x \\in \\R^n$ uniquely determined by the first-order necessary optimality condition $A x = b$, and this critical point is a global minimizer of $J$.\n",
    "\n",
    "For any $e \\in \\R^n$ we have\n",
    "$$\\begin{align*}\n",
    "J(x+e) - J(x) &= \\frac{1}{2} (x+e)^\\top A (x+e) - b^\\top (x+e) - \\frac{1}{2} x^\\top A x + b^\\top x \\\\\n",
    "&= \\frac{1}{2} e^\\top A e + x^\\top A e - b^\\top \\\\\n",
    "&\\overset{r(x) = 0}{=} \\frac{1}{2} e^\\top A e,\n",
    "\\end{align*}$$\n",
    "where the linear part vanishes due to the optimality of $x$.\n",
    "\n",
    "Since $A$ is symmetric positive definite, the functionr $\\norm{\\cdot}_A : \\R^n \\to \\R$ given by $\\norm{u}_A = \\sqrt{u^\\top A u}$ for all $u \\in \\R^n$ is a norm on $\\R^n$. What we established above ican therefore be recast as follows $J(x+e) - J(x) = \\frac{1}{2} \\norm{e}_A^2$ for all $e \\in \\R^n$.\n",
    "\n",
    "This shows that the $A$-norm of a vector is, up to the factor of $\\frac{1}{2}$, the perturbation of the minimum value of $J$ corresponding to the perturbation of the minimizer by that vector.\n",
    "\n",
    "> But why this functional and why this norm?\n",
    "\n",
    "The thing is that many linear equations (partial differential ones or discretizing those algebraic ones) are optimality conditions for suitable functionals (compare Newton's laws in statics and the potential-energy description).\n",
    "\n",
    "In other words, a linear system $A x = b$ with a symmetric positive definite matrix $A$ may well be interesting not in its own right but as a characterization of the exact minimizer of the corresponding functional $J$ defined above. If the problem consists in minimizing a problem-meaningful quantity $J$ (which may represent, for example, the energy of the modelled system), it is immediately clear how to assess appriximate solutions: the accuracy is to be measured by the deviation of the value of $J$ from the exact minimum. For example if energy is what matters to a model, then we should seek approximate solutions that are accurate in this energy sense and not in the sense of some abstract (or just most conventional) vector norm. So instead of standard vector norms, we may need to use the $A$-norm as above shows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-type methods for systems with SPD matrices\n",
    "\n",
    "Consider the Richardson method for $A x = b$ with $A \\in \\R^{n \\times n}$ SPD and $b \\in \\R^n$. As we observed above, $r_{k-1} = - \\nabla J(x_{k-1})$ for all $k \\in \\N$, so the method can be recast as follows\n",
    "\n",
    "$$ x_k = x_{k-1} - \\alpha_k \\nabla J(x_{k-1}) \\quad \\forall k \\in \\N $$\n",
    "\n",
    "This iteration is known as **gradient descent** for the minimization of the functional $J$.\n",
    "\n",
    "At each iteration $k \\in \\N$, the corresponding relaxation parameter $\\alpha_k$ (which sometimes is referred to as the acceleration rate or learning rate of the iteration) may be adapted according to various strategies. For quadratic functionals, one natural choise, giving rise to the **steepest gradient descent** method, consists in choosing $\\alpha_k$ so as to minimize $J$ along the direction $\\nabla J(x_{k-1})$:\n",
    "\n",
    "$$ J(x_{k-1} + \\alpha r_{k-1}) = \\frac{1}{2} \\alpha^2 r_{k-1}^\\top A r_{k-1} + \\alpha r_{k-1}^\\top A x_{k-1} + \\frac{1}{2} x_{k-1}^\\top A x_{k-1} - \\alpha b^\\top r_{k-1} - b^\\top x_{k-1} $$\n",
    "\n",
    "and minimization w.r.t. $\\alpha \\in \\R$ yields the minimizer\n",
    "\n",
    "$$ \\alpha_k = \\frac{(b - A x_{k-1})^\\top r_{k-1}}{r_{k-1}^\\top A r_{k-1}} = \\frac{r_{k-1}^\\top r_{k-1}}{r_{k-1}^\\top A r_{k-1}} = \\frac{\\norm{r_{k-1}}_2^2}{\\norm{r_{k-1}}_A^2} $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
