{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7\n",
    "\n",
    "$$\n",
    "\\global\\let\\phi=\\varphi\n",
    "\\global\\let\\eps=\\varepsilon\n",
    "\\gdef\\F{\\mathbb{F}}\n",
    "\\gdef\\L{\\mathcal{L}}\n",
    "\\gdef\\K{\\mathcal{K}}\n",
    "\\gdef\\P{\\mathcal{P}}\n",
    "\\gdef\\Q{\\mathcal{Q}}\n",
    "\\gdef\\rank{\\mathrm{rank}\\,}\n",
    "\\gdef\\mat{\\mathrm{mat}}\n",
    "\\gdef\\abs#1{\\left|#1\\right|}\n",
    "\\gdef\\norm#1{\\left\\|#1\\right\\|}\n",
    "\\gdef\\matrix#1{\\begin{pmatrix}#1\\end{pmatrix}}\n",
    "\\gdef\\inner#1#2{\\left \\langle #1 ,\\; #2 \\right \\rangle}\n",
    "\\gdef\\spec{\\bm{\\lambda}}\n",
    "\\gdef\\diag{\\mathrm{diag}}\n",
    "\\gdef\\span#1{\\mathrm{span}\\left\\{#1\\right\\}}\n",
    "\\gdef\\offdiag{\\mathrm{offdiag}}\n",
    "\\gdef\\O{\\mathcal{O}}\n",
    "\\gdef\\nto{\\nrightarrow}\n",
    "\\gdef\\Re{\\mathrm{Re}}\n",
    "\\gdef\\Im{\\mathrm{Im}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Krylov iterative methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition 7.1\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\F^{n \\times n}$, $b \\in \\F^n$. For each $k \\in \\N_0$ the subspace $\\K_k(A,b) = \\span{A^j \\, b}_{j=0}^{k-1}$ of $F^n$ is called the **$k$-th Krylov subspace** of $A$ build on vector $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In particular this definition means $K_0(A, r_0) = \\{0\\}$ with $\\dim \\K_0(A,r_0) = 0$.\n",
    "> \n",
    "> Clearly we have $\\K_k(A,b) = \\K_{k-1}(A,b) + \\span(A^{k-1} \\, b)$ and $\\K_{k-1} \\subset \\K_k$ for all $k \\in \\N$.\n",
    "> \n",
    "> Note also that $\\dim K_K \\le k$ for all $k \\in \\N_0$.\n",
    "> \n",
    "> In fact the definition had long been due because we have been working with Krylov subspaces for a while already. Indeed the Richardson method for $Ax=b$ starting at an initial guess $x_0 \\in \\F^n$ generates iterates $x_k \\in x_0 + \\K_k(A, b-A x_0)$ with $k \\in \\N$.\n",
    "> \n",
    "> For example the Jacobi method iterates on $\\K_k(D^{-1}A,\\, D^{-1} (b-Ax_0))$ and GauÃŸ-Seidel on $\\K_k\\big((D+L)^{-1} U ,\\, (D+L)^{-1} (b-Ax_0) \\big)$.\n",
    "> \n",
    "> All methods operating on the sequence of Krylov subspaces may be called **Krylov methods** in the broad sense. The methods we have considered so far are, however, restricted in what part of the $k$-th Krylov subspace they can use at step $k$. We will now turn to methods that use Krylov subspaces more flexibly and can be called Krylov methods in a more narrow sense.\n",
    "> \n",
    "> Directly from definition 7.1 it followes that $\\K_k(A,b)$ is parametrized by $\\P_{k-1}$ for every $k \\in \\N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proposition 7.2\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\F^{n \\times n}$, $b \\in \\F^n$ and $k \\in \\N$. Then $\\K_k(A,b) = \\{\\pi(A) b \\ | \\ \\pi \\in \\P_{k-1} \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Revisiting remark 6.6 we note that the relation between $\\pi_k$ and $q_k$ for $k \\in \\N_0$ can be twisted: $\\pi_k$ for $k \\in \\N_0$ can be chosesn first and $q_k$ can be derived consistently with that choice, as we will show now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 7.3\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\F^{n \\times n}$, $b \\in \\F^n$, $x_0 \\in \\F^n$, $r_0 = b - A x_0$. Consider $k \\in \\N_0$.\n",
    "\n",
    "- If $k = 0$ let $\\pi_k = 0 \\in \\P_0$. If $k \\in \\N$ assume that $\\pi_k \\in \\P_{k-1}$ is of degree $k-1$.  \n",
    "Consider $x_k = x_0 + \\pi_k(A) r_0$. Then $e_k = x_k -  = \\big( I - \\pi_k(A) A \\big) (x_0 - x) = q_k(A) e_0$ and $r_k = q_k(A) r_0$ for all $k \\in \\N_0$, where $q_0 = 1 \\in \\P_0$ and $q_k \\in \\P_k$ is given by $q_k(t) = 1 - t \\pi_k(t)$ for all $t \\in \\F$.  \n",
    "This relation implies $q_k(0) = 1$, i.e. $q_k \\in \\Q_k$ for all $k \\in \\N_0$.  \n",
    "So any choice of $\\pi_k \\in \\P_{k-1}$ of degree $k-1$ uniquely defines $q_k \\in \\Q_k$ such that $x_k - x_0 = \\pi_k(A) r_0$ implies $e_k = q_k(A) e_0$.\n",
    "\n",
    "- On the other hand if $q_k \\in \\Q_k$ then $\\pi_k(t) = \\frac{1}{t} \\big( 1 - q_k(t) \\big)$ for all $t \\in \\F$ defines $\\pi_k \\in \\P_{k-1}$.  \n",
    "As a result any choice of $q_k \\in \\Q_k$ uniquely defines $\\pi_k \\in \\P_{k-1}$ of degree $k-1$ such that $x_k - x_0 = \\pi_k(A) r_0$ implies $e_k = q_k(A) e_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate-gradient method\n",
    "\n",
    "The first Krylov method (in the narrow sense) we will consider is the conjugate-gradient method.\n",
    "\n",
    "We assume that $n \\in \\N$, $A \\in \\F^{n \\times n}$ is a Hermitian positive-definite (HPD) matrix, $b, x_0 \\in \\F^n$.\n",
    "\n",
    "Since $A$ is HPD the function $\\inner{\\cdot}{\\cdot}_A : \\F^n \\times \\F^n \\to \\F$ given by $\\inner{u}{v}_A = u^* A v$ for all $u, v \\in \\F^n$ is an inner product on $F^n$.\n",
    "\n",
    "This inner product is called the **inner product induced by matrix** $A$ or shorter, the **$A$-inner product**. Vectors orthogonal in the sense of the $A$-inner product are said to be **$A$-othogonal**.\n",
    "\n",
    "The **conjugate-gradient method** for $Ax=b$ starting at $x_0$ generates $x_k \\in \\F^n$ for each $k \\in \\N$ such that $$ x_k - x_0 = \\argmin_{y \\in \\K_k(A, r_0)} \\norm{y - (x - x_0)}_A $$ in an $A$-orthogonal basis of $\\K_k(A, r_0)$.\n",
    "\n",
    "This completely defines the method and the formulas we will derive very soon. In short, the method is identified by\n",
    "- the goal of minimizing the $A$-norm of the error,\n",
    "- the solution subspaces $K_k(A, r_0)$ and\n",
    "- the $A$-orthogonality of the bases used for $K_k(A, r_0)$.\n",
    "\n",
    "Let us set $y_k = \\argmin_{y \\in \\K_k} \\norm{y - (x - x_0)}_A$ for all $k \\in \\N_0$ so that $x_k = x_0 + y_k$ for $k \\in \\N$. Our goal is to construct $p_1, \\dots, p_k$ thatform an $A$-orthogonal basis for $\\K_k(A, r_0)$. This is possible if and only if $\\dim \\K_k(A, r_0) = k$.\n",
    "\n",
    "First let us prove an auxiliary result characterizing the iterates of the CG method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemma 7.4\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\F^{n \\times n}$ be HPD, $r_0 \\in \\F^n$, $k \\in \\N$, $y_k = \\argmin_{u \\in \\K_k(A, r_0)} \\norm{u - A^{-1} r_0}_A$ and $r_k = r_0 - A y_k$. Then $r_k \\perp \\K_k(A, r_0)$.\n",
    "\n",
    "##### Proof\n",
    "\n",
    "The optimality of $y_k = \\argmin_{u \\in \\K_k(A, r_0)} \\norm{u - A^{-1} r_0}_A$ is characterized by $y_k - A^{-1} r_0 \\perp_A \\K_k(A, r_0)$, i.e., $A (y_k - A^{-1} r_0) \\perp K_k(A, r_0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemma 7.5\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\F^{n \\times n}$ be HPD, $r_0 \\in \\F^n$, $k \\in \\N_0$, $r_k \\in \\K_{k+1}(A, r_0)$ be a non-zero vector orthogonal to $K_k(A, r_0)$, $p_{k+1} \\in \\K_{k+1}(A, r_0)$ be a non-zero vector $A$-orthogonal to $K_k(A, r_0)$.\n",
    "\n",
    "Then $p_1 = \\gamma_0 r_0$ and $p_{k+1} = \\gamma_k (r_k + \\beta_k p_k)$ with $\\gamma_k = \\frac{r_k^* p_{k+1}}{r_k^* r_k}$ and $\\beta_k = - \\frac{p_k^* A r_k}{p_k^* A p_k}$\n",
    "\n",
    "##### Proof\n",
    "\n",
    "Since $\\dim \\K_{k+1} \\le \\dim K_k + 1$ and $r_k \\in \\K_{k+1} \\setminus K_k$ we have $\\K_{k+1} = \\K_k + \\span{r_k}$.\n",
    "\n",
    "When $k = 0$ this gives $\\K_1 = \\span{r_0}$ so that $p_1 \\in \\span{r_0}$. Then the coefficient of $p_1$ along $r_0$ is $\\gamma_0$, so $p_1 = \\gamma_0 r_0$.\n",
    "\n",
    "When $k \\in \\K$ we have $p_k \\in \\K_k \\setminus K_{k-1}$ so since $\\dim \\K_k \\le \\dim \\K_{k-1} + 1$, we conclude that $\\K_k = \\K_{k-1} + \\span{p_k}$ and hence $\\K_{k+1} = \\K_{k-1} + \\span{p_k,\\, r_k}$.\n",
    "\n",
    "Due to $p_{k+1} \\in \\K_{k+1}$ there exists $u_k \\in \\K_{k-1}$ and $\\mu_k, \\nu_k \\in \\F$ such that $p_{k+1} = u_k + \\mu_k r_k + \\nu_k p_k$.\n",
    "\n",
    "Due to $A \\K_{k-1} \\subset \\K_k$ it holds that $r_k \\perp_A \\K_{k-1}$. Further we have $p_k \\perp_A \\K_{k-1}$.\n",
    "\n",
    "Finally recall $p_{k+1} \\perp_A \\K_k$ and hence $p_{k+1} \\perp_A \\K_{k-1}$. This yields $u_k = p_{k+1} - \\mu_k r_k - \\nu_k p_k \\perp_A \\K_{k-1}$ and therefore $u_k = 0$.\n",
    "\n",
    "Projecting $p_{k+1}$ onto $p_k$ w.r.t. the $A$-inner product and using $p_{k+1} \\perp_A \\K_k$ we get $0 = \\mu_k p_k^* A r_k + \\nu_k p_k^* A p_k$.\n",
    "\n",
    "Projecting $p_{k+1}$ onto $r_k$ w.r.t. the standard inner product and using $r_k \\perp \\K_k$ we get $r_k^* p_{k+1} = \\mu_k r_k^* r_k$ so that $\\mu_k = \\gamma_k$.\n",
    "\n",
    "Then $\\nu_k = - \\mu_k \\frac{p_k^* A r_k}{p_k^* A p_k} = \\gamma_k \\beta_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that $\\gamma_k$ in lemma 7.5 is merely a normalization coefficient which is non-zero ($\\gamma_k = 0$ would imply $r_k \\perp \\K_{k+1}$ for $r_k \\in \\K_{k+1}$, i.e. $r_k = 0$) and can be made equal to 1 by resacling $p_{k+1}$.\n",
    "> \n",
    "> Finally we can consider the update introduced at iteration $k \\in \\N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemma 7.6\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\F^{n \\times n}$ be HPD, $r_0 \\in \\F^n$, $k \\in \\N$, $y_i = \\argmin_{y \\in \\K_i(A, r_0)} \\norm{y - A^{-1} r_0}_A$, $r_i = r_0 - A y_i$ for $i \\in \\{k-1, k\\}$, $p_k \\in \\K_k(A, r_0) be non-zero and $A$-orthogonal to $\\K_{k-1}(A,r_0)$.\n",
    "\n",
    "Then $y_k = y_{k-1} + \\alpha_k p_k$ holds with $\\alpha_k = \\frac{p_k^* r_{k-1}}{p_k^* A p_k}$.\n",
    "\n",
    "##### Proof\n",
    "\n",
    "Since $\\dim \\K_k \\le \\dim \\K_{k-1} + 1$ and $p_k \\in \\K_k \\setminus \\K_{k-1}$ we have $\\dim \\K_k = \\dim \\K_{k-1} + 1$. So $K_k = K_{k-1} +_{\\perp_A} \\span{p_k}$ (an $A$-orthogonal space decomposition).\n",
    "\n",
    "Then since $y_k$ and $y_{k-1}$ are $A$-orthogonal projections of $A^{-1} r_0$ onto $\\K_k$ and $\\K_{k-1}$ respectively, we have $y_k = y_{k-1} + \\alpha_k p_k$ with $\\alpha_k = \\frac{p_k^* A A^{-1} r_0}{p_k^* A p_k} = \\frac{p_k^* r_0}{p_k^* A p_k}$.\n",
    "\n",
    "Since $r_k \\perp \\K_k$ by lemma 7.4, $r_{k-1} = r_0 - A y_{k-1}$ and $A y_{k-1} \\in A \\K_{k-1} \\subset \\K_k$ we have $p_k^* r_0 = p_k^* r_{k-1}$.\n",
    "\n",
    "So we have $\\alpha_k = \\frac{p_k^* r_{k-1}}{p_k^* A p_k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem 7.7\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\F^{n \\times n}$ be HPD, $r_0 \\in \\F^n$, $m \\in \\N$, $r_k = r_0 - A y_k$ for all $k \\in \\{1, \\dots, m\\}$, $r_{k-1} \\ne 0$ for all $k \\in \\{1, \\dots, m\\}$, $p_1, \\dots, p_m \\in \\F^n$ be $A$-orthogonal and such that $p_k^* r_{k-1} = r_{k-1}^* r_{k-1}$ for all $k \\in \\{1, \\dots, m\\}$ and $p_1, \\dots, p_k$ is a basis for $\\K_k(A, r_0)$ for all $k \\in \\{1, \\dots, m\\}$, $y_k = \\argmin_{u \\in \\K_k(A,r_0)} \\norm{u - A^{-1} r_0}_A$ for all $k \\in \\{1, \\dots, m\\}$.\n",
    "\n",
    "Then $y_k = y_{k-1} + \\alpha_k p_k$ and $r_k = r_{k-1} - \\alpha_k A p_k$ with $\\alpha_k = \\frac{r_{k-1}^* r_{k-1}}{p_k^* A p_k}$ for all $k \\in \\{1, \\dots, m\\}$ and $p_{k+1} = r_k + \\beta_k p_k$ with $\\beta_k = \\frac{r_k^* r_k}{r_{k-1}^* r_{k-1}}$ for all $k \\in \\{1, \\dots, m-1\\}$.\n",
    "\n",
    "##### Proof\n",
    "\n",
    "By lemma 7.6 we have $y_k = y_{k-1} + \\alpha_k p_k$ with $\\alpha_k = \\frac{p_k^* r_{k-1}}{p_k^* A p_k} = \\frac{r_{k-1}^* r_{k-1}}{p_k^* A p_k}$ for all $k \\in \\{1, \\dots, m\\}$. This implies $r_k = r_{k-1} - A (x_k - x_{k-1}) = r_{k-1} - \\alpha_k A p_k$ for all $k \\in \\{1, \\dots, m\\}$.\n",
    "\n",
    "Finally consider $k \\in \\{1, \\dots, m-1\\}$. By lemma 7.5 we have $p_{k+1} = r_k + \\beta_k p_k$ with $\\beta_k = - \\frac{p_k^* A r_k}{p_k^* A p_k}$. Since $r_{k-1} \\ne 0$ by assumption we have $\\alpha_k \\ne 0$ and hence $A p_k = \\frac{1}{\\alpha_k} (r_{k-1} - r_k)$.\n",
    "\n",
    "Then $p_k^* A r_k = \\frac{1}{\\alpha_k} (r_{k-1} - r_k)^* r_k = - \\frac{1}{\\alpha_k} r_k^* r_k$ since $r_k \\perp \\K_k$ by lemma 7.4 and $r_{k-1} \\in \\K_{k-1}$.\n",
    "\n",
    "So $\\beta_k = \\frac{r_k^* r_k}{p_k^* A p_k} \\frac{1}{\\alpha_k} = \\frac{r_k^* r_k}{r_{k-1}^* r_{k-1}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The CG algorithm therefore takes the following practical form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm 7.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CG (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "function CG(A::Matrix{S}, b::Vector{T}, x0::Vector{U}, tol::Real=1e-8) where {S, T, U}\n",
    "\n",
    "    # check input dimensions\n",
    "    m, n = size(A)\n",
    "    n_b, = size(b)\n",
    "    n_x0, = size(x0)\n",
    "    if m != n\n",
    "        throw(ArgumentError(\"matrix has to be square: got $m by $n\"))\n",
    "    elseif n != n_b\n",
    "        throw(ArgumentError(\"system vector has to be of size $n: got $n_b\"))\n",
    "    elseif n != n_x0\n",
    "        throw(ArgumentError(\"initial vector has to be of size $n: got $n_x0\"))\n",
    "    end\n",
    "\n",
    "    # initialization\n",
    "    r0 = b - A * x0\n",
    "    p = r0\n",
    "\n",
    "    while true\n",
    "        alpha = dot(r0, r0) / dot(p, A * p)\n",
    "        x1 = x0 + alpha * p\n",
    "        r1 = r0 - alpha * A * p\n",
    "        if isapprox(r1, 0.0, atol=tol, rtol=0)\n",
    "            break\n",
    "        end\n",
    "        p = r1 + dot(r1, r1) / dot(r0, r0) * p\n",
    "        x0 = x1\n",
    "        r0 = r1\n",
    "    end\n",
    "\n",
    "    return x1\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching isapprox(::Vector{Float64}, ::Float64; atol=1.0e-8, rtol=0)\nClosest candidates are:\n  isapprox(!Matched::Number, ::Number; atol, rtol, nans, norm) at floatfuncs.jl:300\n  isapprox(::AbstractArray, !Matched::AbstractArray; atol, rtol, nans, norm) at /opt/julia-1.8.0/share/julia/stdlib/v1.8/LinearAlgebra/src/generic.jl:1696\n  isapprox(!Matched::Missing, ::Any; kwargs...) at missing.jl:90\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching isapprox(::Vector{Float64}, ::Float64; atol=1.0e-8, rtol=0)\n",
      "Closest candidates are:\n",
      "  isapprox(!Matched::Number, ::Number; atol, rtol, nans, norm) at floatfuncs.jl:300\n",
      "  isapprox(::AbstractArray, !Matched::AbstractArray; atol, rtol, nans, norm) at /opt/julia-1.8.0/share/julia/stdlib/v1.8/LinearAlgebra/src/generic.jl:1696\n",
      "  isapprox(!Matched::Missing, ::Any; kwargs...) at missing.jl:90\n",
      "  ...\n",
      "\n",
      "Stacktrace:\n",
      " [1] CG(A::Matrix{Int64}, b::Vector{Int64}, x0::Vector{Float64}, tol::Float64)\n",
      "   @ Main ~/Repos/Advanced-Numerical-Analysis-Course/lecture/lecture_07.ipynb:25\n",
      " [2] CG(A::Matrix{Int64}, b::Vector{Int64}, x0::Vector{Float64})\n",
      "   @ Main ~/Repos/Advanced-Numerical-Analysis-Course/lecture/lecture_07.ipynb:6\n",
      " [3] top-level scope\n",
      "   @ ~/Repos/Advanced-Numerical-Analysis-Course/lecture/lecture_07.ipynb:5"
     ]
    }
   ],
   "source": [
    "n = 3;\n",
    "A = collect(reshape(1:(n^2), (n,n)));\n",
    "b = collect(1:n);\n",
    "x0 = zeros(n);\n",
    "CG(A, b, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a, = size(b)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
