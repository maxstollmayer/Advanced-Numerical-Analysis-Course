{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7\n",
    "\n",
    "$$\n",
    "\\global\\let\\phi=\\varphi\n",
    "\\global\\let\\eps=\\varepsilon\n",
    "\\gdef\\F{\\mathbb{F}}\n",
    "\\gdef\\L{\\mathcal{L}}\n",
    "\\gdef\\K{\\mathcal{K}}\n",
    "\\gdef\\P{\\mathcal{P}}\n",
    "\\gdef\\Q{\\mathcal{Q}}\n",
    "\\gdef\\rank{\\mathrm{rank}\\,}\n",
    "\\gdef\\mat{\\mathrm{mat}}\n",
    "\\gdef\\abs#1{\\left|#1\\right|}\n",
    "\\gdef\\norm#1{\\left\\|#1\\right\\|}\n",
    "\\gdef\\matrix#1{\\begin{pmatrix}#1\\end{pmatrix}}\n",
    "\\gdef\\inner#1#2{\\left \\langle #1 ,\\; #2 \\right \\rangle}\n",
    "\\gdef\\spec{\\bm{\\lambda}}\n",
    "\\gdef\\diag{\\mathrm{diag}}\n",
    "\\gdef\\span#1{\\mathrm{span}\\left\\{#1\\right\\}}\n",
    "\\gdef\\offdiag{\\mathrm{offdiag}}\n",
    "\\gdef\\O{\\mathcal{O}}\n",
    "\\gdef\\nto{\\nrightarrow}\n",
    "\\gdef\\Re{\\mathrm{Re}}\n",
    "\\gdef\\Im{\\mathrm{Im}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Krylov iterative methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition 7.1\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\F^{n \\times n}$, $b \\in \\F^n$. For each $k \\in \\N_0$ the subspace $\\K_k(A,b) = \\span{A^j \\, b}_{j=0}^{k-1}$ of $F^n$ is called the **$k$-th Krylov subspace** of $A$ build on vector $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In particular this definition means $K_0(A, r_0) = \\{0\\}$ with $\\dim \\K_0(A,r_0) = 0$.\n",
    "> \n",
    "> Clearly we have $\\K_k(A,b) = \\K_{k-1}(A,b) + \\span(A^{k-1} \\, b)$ and $\\K_{k-1} \\subset \\K_k$ for all $k \\in \\N$.\n",
    "> \n",
    "> Note also that $\\dim K_K \\le k$ for all $k \\in \\N_0$.\n",
    "> \n",
    "> In fact the definition had long been due because we have been working with Krylov subspaces for a while already. Indeed the Richardson method for $Ax=b$ starting at an initial guess $x_0 \\in \\F^n$ generates iterates $x_k \\in x_0 + \\K_k(A, b-A x_0)$ with $k \\in \\N$.\n",
    "> \n",
    "> For example the Jacobi method iterates on $\\K_k(D^{-1}A,\\, D^{-1} (b-Ax_0))$ and GauÃŸ-Seidel on $\\K_k\\big((D+L)^{-1} U ,\\, (D+L)^{-1} (b-Ax_0) \\big)$.\n",
    "> \n",
    "> All methods operating on the sequence of Krylov subspaces may be called **Krylov methods** in the broad sense. The methods we have considered so far are, however, restricted in what part of the $k$-th Krylov subspace they can use at step $k$. We will now turn to methods that use Krylov subspaces more flexibly and can be called Krylov methods in a more narrow sense.\n",
    "> \n",
    "> Directly from definition 7.1 it followes that $\\K_k(A,b)$ is parametrized by $\\P_{k-1}$ for every $k \\in \\N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proposition 7.2\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\F^{n \\times n}$, $b \\in \\F^n$ and $k \\in \\N$. Then $\\K_k(A,b) = \\{\\pi(A) b \\ | \\ \\pi \\in \\P_{k-1} \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Revisiting remark 6.6 we note that the relation between $\\pi_k$ and $q_k$ for $k \\in \\N_0$ can be twisted: $\\pi_k$ for $k \\in \\N_0$ can be chosesn first and $q_k$ can be derived consistently with that choice, as we will show now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 7.3\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\F^{n \\times n}$, $b \\in \\F^n$, $x_0 \\in \\F^n$, $r_0 = b - A x_0$. Consider $k \\in \\N_0$.\n",
    "\n",
    "- If $k = 0$ let $\\pi_k = 0 \\in \\P_0$. If $k \\in \\N$ assume that $\\pi_k \\in \\P_{k-1}$ is of degree $k-1$.  \n",
    "Consider $x_k = x_0 + \\pi_k(A) r_0$. Then $e_k = x_k -  = \\big( I - \\pi_k(A) A \\big) (x_0 - x) = q_k(A) e_0$ and $r_k = q_k(A) r_0$ for all $k \\in \\N_0$, where $q_0 = 1 \\in \\P_0$ and $q_k \\in \\P_k$ is given by $q_k(t) = 1 - t \\pi_k(t)$ for all $t \\in \\F$.  \n",
    "This relation implies $q_k(0) = 1$, i.e. $q_k \\in \\Q_k$ for all $k \\in \\N_0$.  \n",
    "So any choice of $\\pi_k \\in \\P_{k-1}$ of degree $k-1$ uniquely defines $q_k \\in \\Q_k$ such that $x_k - x_0 = \\pi_k(A) r_0$ implies $e_k = q_k(A) e_0$.\n",
    "\n",
    "- On the other hand if $q_k \\in \\Q_k$ then $\\pi_k(t) = \\frac{1}{t} \\big( 1 - q_k(t) \\big)$ for all $t \\in \\F$ defines $\\pi_k \\in \\P_{k-1}$.  \n",
    "As a result any choice of $q_k \\in \\Q_k$ uniquely defines $\\pi_k \\in \\P_{k-1}$ of degree $k-1$ such that $x_k - x_0 = \\pi_k(A) r_0$ implies $e_k = q_k(A) e_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate-gradient method\n",
    "\n",
    "The first Krylov method (in the narrow sense) we will consider is the conjugate-gradient method.\n",
    "\n",
    "We assume that $n \\in \\N$, $A \\in \\F^{n \\times n}$ is a Hermitian positive-definite (HPD) matrix, $b, x_0 \\in \\F^n$.\n",
    "\n",
    "Since $A$ is HPD the function $\\inner{\\cdot}{\\cdot}_A : \\F^n \\times \\F^n \\to \\F$ given by $\\inner{u}{v}_A = u^* A v$ for all $u, v \\in \\F^n$ is an inner product on $F^n$.\n",
    "\n",
    "This inner product is called the **inner product induced by matrix** $A$ or shorter, the **$A$-inner product**. Vectors orthogonal in the sense of the $A$-inner product are said to be **$A$-othogonal**.\n",
    "\n",
    "The **conjugate-gradient method** for $Ax=b$ starting at $x_0$ generates $x_k \\in \\F^n$ for each $k \\in \\N$ such that $$ x_k - x_0 = \\argmin_{y \\in \\K_k(A, r_0)} \\norm{y - (x - x_0)}_A $$ in an $A$-orthogonal basis of $\\K_k(A, r_0)$.\n",
    "\n",
    "This completely defines the method and the formulas we will derive very soon. In short, the method is identified by\n",
    "- the goal of minimizing the $A$-norm of the error,\n",
    "- the solution subspaces $K_k(A, r_0)$ and\n",
    "- the $A$-orthogonality of the bases used for $K_k(A, r_0)$.\n",
    "\n",
    "Let us set $y_k = \\argmin_{y \\in \\K_k} \\norm{y - (x - x_0)}_A$ for all $k \\in \\N_0$ so that $x_k = x_0 + y_k$ for $k \\in \\N$. Our goal is to construct $p_1, \\dots, p_k$ thatform an $A$-orthogonal basis for $\\K_k(A, r_0)$. This is possible if and only if $\\dim \\K_k(A, r_0) = k$.\n",
    "\n",
    "First let us prove an auxiliary result characterizing the iterates of the CG method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemma 7.4\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\F^{n \\times n}$ be HPD, $r_0 \\in \\F^n$, $k \\in \\N$, $y_k = \\argmin_{u \\in \\K_k(A, r_0)} \\norm{u - A^{-1} r_0}_A$ and $r_k = r_0 - A y_k$. Then $r_k \\perp \\K_k(A, r_0)$.\n",
    "\n",
    "##### Proof\n",
    "\n",
    "The optimality of $y_k = \\argmin_{u \\in \\K_k(A, r_0)} \\norm{u - A^{-1} r_0}_A$ is characterized by $y_k - A^{-1} r_0 \\perp_A \\K_k(A, r_0)$, i.e., $A (y_k - A^{-1} r_0) \\perp K_k(A, r_0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemma 7.5\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\F^{n \\times n}$ be HPD, $r_0 \\in \\F^n$, $k \\in \\N_0$, $r_k \\in \\K_{k+1}(A, r_0)$ be a non-zero vector orthogonal to $K_k(A, r_0)$, $p_{k+1} \\in \\K_{k+1}(A, r_0)$ be a non-zero vector $A$-orthogonal to $K_k(A, r_0)$.\n",
    "\n",
    "Then $p_1 = \\gamma_0 r_0$ and $p_{k+1} = \\gamma_k (r_k + \\beta_k p_k)$ with $\\gamma_k = \\frac{r_k^* p_{k+1}}{r_k^* r_k}$ and $\\beta_k = - \\frac{p_k^* A r_k}{p_k^* A p_k}$\n",
    "\n",
    "##### Proof\n",
    "\n",
    "Since $\\dim \\K_{k+1} \\le \\dim K_k + 1$ and $r_k \\in \\K_{k+1} \\setminus K_k$ we have $\\K_{k+1} = \\K_k + \\span{r_k}$.\n",
    "\n",
    "When $k = 0$ this gives $\\K_1 = \\span{r_0}$ so that $p_1 \\in \\span{r_0}$. Then the coefficient of $p_1$ along $r_0$ is $\\gamma_0$, so $p_1 = \\gamma_0 r_0$.\n",
    "\n",
    "When $k \\in \\K$ we have $p_k \\in \\K_k \\setminus K_{k-1}$ so since $\\dim \\K_k \\le \\dim \\K_{k-1} + 1$, we conclude that $\\K_k = \\K_{k-1} + \\span{p_k}$ and hence $\\K_{k+1} = \\K_{k-1} + \\span{p_k,\\, r_k}$.\n",
    "\n",
    "Due to $p_{k+1} \\in \\K_{k+1}$ there exists $u_k \\in \\K_{k-1}$ and $\\mu_k, \\nu_k \\in \\F$ such that $p_{k+1} = u_k + \\mu_k r_k + \\nu_k p_k$.\n",
    "\n",
    "Due to $A \\K_{k-1} \\subset \\K_k$ it holds that $r_k \\perp_A \\K_{k-1}$. Further we have $p_k \\perp_A \\K_{k-1}$.\n",
    "\n",
    "Finally recall $p_{k+1} \\perp_A \\K_k$ and hence $p_{k+1} \\perp_A \\K_{k-1}$. This yields $u_k = p_{k+1} - \\mu_k r_k - \\nu_k p_k \\perp_A \\K_{k-1}$ and therefore $u_k = 0$.\n",
    "\n",
    "Projecting $p_{k+1}$ onto $p_k$ w.r.t. the $A$-inner product and using $p_{k+1} \\perp_A \\K_k$ we get $0 = \\mu_k p_k^* A r_k + \\nu_k p_k^* A p_k$.\n",
    "\n",
    "Projecting $p_{k+1}$ onto $r_k$ w.r.t. the standard inner product and using $r_k \\perp \\K_k$ we get $r_k^* p_{k+1} = \\mu_k r_k^* r_k$ so that $\\mu_k = \\gamma_k$.\n",
    "\n",
    "Then $\\nu_k = - \\mu_k \\frac{p_k^* A r_k}{p_k^* A p_k} = \\gamma_k \\beta_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that $\\gamma_k$ in lemma 7.5 is merely a normalization coefficient which is non-zero ($\\gamma_k = 0$ would imply $r_k \\perp \\K_{k+1}$ for $r_k \\in \\K_{k+1}$, i.e. $r_k = 0$) and can be made equal to 1 by resacling $p_{k+1}$.\n",
    "> \n",
    "> Finally we can consider the update introduced at iteration $k \\in \\N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemma 7.6\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\F^{n \\times n}$ be HPD, $r_0 \\in \\F^n$, $k \\in \\N$, $y_i = \\argmin_{y \\in \\K_i(A, r_0)} \\norm{y - A^{-1} r_0}_A$, $r_i = r_0 - A y_i$ for $i \\in \\{k-1, k\\}$, $p_k \\in \\K_k(A, r_0) be non-zero and $A$-orthogonal to $\\K_{k-1}(A,r_0)$.\n",
    "\n",
    "Then $y_k = y_{k-1} + \\alpha_k p_k$ holds with $\\alpha_k = \\frac{p_k^* r_{k-1}}{p_k^* A p_k}$.\n",
    "\n",
    "##### Proof\n",
    "\n",
    "Since $\\dim \\K_k \\le \\dim \\K_{k-1} + 1$ and $p_k \\in \\K_k \\setminus \\K_{k-1}$ we have $\\dim \\K_k = \\dim \\K_{k-1} + 1$. So $K_k = K_{k-1} +_{\\perp_A} \\span{p_k}$ (an $A$-orthogonal space decomposition).\n",
    "\n",
    "Then since $y_k$ and $y_{k-1}$ are $A$-orthogonal projections of $A^{-1} r_0$ onto $\\K_k$ and $\\K_{k-1}$ respectively, we have $y_k = y_{k-1} + \\alpha_k p_k$ with $\\alpha_k = \\frac{p_k^* A A^{-1} r_0}{p_k^* A p_k} = \\frac{p_k^* r_0}{p_k^* A p_k}$.\n",
    "\n",
    "Since $r_k \\perp \\K_k$ by lemma 7.4, $r_{k-1} = r_0 - A y_{k-1}$ and $A y_{k-1} \\in A \\K_{k-1} \\subset \\K_k$ we have $p_k^* r_0 = p_k^* r_{k-1}$.\n",
    "\n",
    "So we have $\\alpha_k = \\frac{p_k^* r_{k-1}}{p_k^* A p_k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem 7.7\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\F^{n \\times n}$ be HPD, $r_0 \\in \\F^n$, $m \\in \\N$, $r_k = r_0 - A y_k$ for all $k \\in \\{1, \\dots, m\\}$, $r_{k-1} \\ne 0$ for all $k \\in \\{1, \\dots, m\\}$, $p_1, \\dots, p_m \\in \\F^n$ be $A$-orthogonal and such that $p_k^* r_{k-1} = r_{k-1}^* r_{k-1}$ for all $k \\in \\{1, \\dots, m\\}$ and $p_1, \\dots, p_k$ is a basis for $\\K_k(A, r_0)$ for all $k \\in \\{1, \\dots, m\\}$, $y_k = \\argmin_{u \\in \\K_k(A,r_0)} \\norm{u - A^{-1} r_0}_A$ for all $k \\in \\{1, \\dots, m\\}$.\n",
    "\n",
    "Then $y_k = y_{k-1} + \\alpha_k p_k$ and $r_k = r_{k-1} - \\alpha_k A p_k$ with $\\alpha_k = \\frac{r_{k-1}^* r_{k-1}}{p_k^* A p_k}$ for all $k \\in \\{1, \\dots, m\\}$ and $p_{k+1} = r_k + \\beta_k p_k$ with $\\beta_k = \\frac{r_k^* r_k}{r_{k-1}^* r_{k-1}}$ for all $k \\in \\{1, \\dots, m-1\\}$.\n",
    "\n",
    "##### Proof\n",
    "\n",
    "By lemma 7.6 we have $y_k = y_{k-1} + \\alpha_k p_k$ with $\\alpha_k = \\frac{p_k^* r_{k-1}}{p_k^* A p_k} = \\frac{r_{k-1}^* r_{k-1}}{p_k^* A p_k}$ for all $k \\in \\{1, \\dots, m\\}$. This implies $r_k = r_{k-1} - A (x_k - x_{k-1}) = r_{k-1} - \\alpha_k A p_k$ for all $k \\in \\{1, \\dots, m\\}$.\n",
    "\n",
    "Finally consider $k \\in \\{1, \\dots, m-1\\}$. By lemma 7.5 we have $p_{k+1} = r_k + \\beta_k p_k$ with $\\beta_k = - \\frac{p_k^* A r_k}{p_k^* A p_k}$. Since $r_{k-1} \\ne 0$ by assumption we have $\\alpha_k \\ne 0$ and hence $A p_k = \\frac{1}{\\alpha_k} (r_{k-1} - r_k)$.\n",
    "\n",
    "Then $p_k^* A r_k = \\frac{1}{\\alpha_k} (r_{k-1} - r_k)^* r_k = - \\frac{1}{\\alpha_k} r_k^* r_k$ since $r_k \\perp \\K_k$ by lemma 7.4 and $r_{k-1} \\in \\K_{k-1}$.\n",
    "\n",
    "So $\\beta_k = \\frac{r_k^* r_k}{p_k^* A p_k} \\frac{1}{\\alpha_k} = \\frac{r_k^* r_k}{r_{k-1}^* r_{k-1}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The CG algorithm therefore takes the following practical form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm 7.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CG (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "function CG(A::Matrix{S}, b::Vector{T}; x0::Union{Vector{U},Nothing}=nothing, tol::Real=1e-8) where {S, T, U}\n",
    "\n",
    "    # check arguments\n",
    "    m, n = size(A)\n",
    "    if m != n\n",
    "        throw(ArgumentError(\"matrix has to be square: got $m by $n\"))\n",
    "    end\n",
    "\n",
    "    n_b, = size(b)\n",
    "    if n != n_b\n",
    "        throw(ArgumentError(\"system vector has to be of size $n: got $n_b\"))\n",
    "    end\n",
    "\n",
    "    if isnothing(x0)\n",
    "        x0 = zeros(n)\n",
    "    else\n",
    "        n_x0, = size(x0)\n",
    "        if n != n_x0\n",
    "            throw(ArgumentError(\"initial vector has to be of size $n: got $n_x0\"))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if tol <= 0\n",
    "        throw(ArgumentError(\"tolerance has to be greater than zero: got $tol\"))\n",
    "    end\n",
    "\n",
    "    # initialization\n",
    "    r0 = b - A * x0\n",
    "    p = r0\n",
    "    alpha = dot(r0, r0) / dot(p, A * p)\n",
    "    x1 = x0 + alpha * p\n",
    "    r1 = r0 - alpha * A * p\n",
    "    k = 0\n",
    "\n",
    "    norm_time = 0\n",
    "    p_time = 0\n",
    "    a_time = 0\n",
    "    x_time = 0\n",
    "    r_time = 0\n",
    "\n",
    "    norms = []\n",
    "\n",
    "    # CG loop\n",
    "    while norm(r1, 1) > tol && k < 100\n",
    "        t = @timed norms = [norms; norm(r1, 1)]\n",
    "        norm_time += t.time\n",
    "\n",
    "        t = @timed begin\n",
    "            p = r1 + dot(r1, r1) / dot(r0, r0) * p\n",
    "        end\n",
    "        p_time += t.time\n",
    "\n",
    "        t = @timed begin\n",
    "            alpha = dot(r0, r0) / dot(p, A * p)\n",
    "        end\n",
    "        a_time += t.time\n",
    "\n",
    "        t = @timed begin\n",
    "            x1 = x0 + alpha * p\n",
    "        end\n",
    "        x_time += t.time\n",
    "\n",
    "        t = @timed begin\n",
    "            r1 = r0 - alpha * A * p\n",
    "        end\n",
    "        r_time += t.time\n",
    "\n",
    "        x0 = x1\n",
    "        r0 = r1\n",
    "        k += 1\n",
    "    end\n",
    "\n",
    "    println(\"norm time: \", norm_time)\n",
    "    println(\"p time: \", p_time)\n",
    "    println(\"a time: \", a_time)\n",
    "    println(\"x time: \", x_time)\n",
    "    println(\"r time: \", r_time)\n",
    "\n",
    "    plot(1:length(norms), norms)\n",
    "\n",
    "    return x1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm time: 6.146899999999999e-5\n",
      "p time: 2.3810000000000004e-5\n",
      "a time: 1.9261000000000004e-5\n",
      "x time: 1.555300000000001e-5\n",
      "r time: 1.4797999999999996e-5\n",
      "Any"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5, 6.951532750363851, 2.3250058300955807, 2.9656079359175527, 4.469377334071989, 2.143497082016751, 2.351802134652032, 3.050627202595532, 6.677144589010064, 1.5572750953018208, 1.952727838397716, 3.0401395140116407, 4.446861724250657, 3.4222861529769357, 5.650679330322976, 1.8094031874584355, 2.236983622448239, 3.938405783475145, 2.6441527544891077, 9.24482516990495, 3.4852308332811903, 4.405951659052011, 4.96758685189306, 3.14862055379726, 2.6932318784348004, 2.620048163923239, 2.661856504790398, 2.8049635440323946, 3.203566307712992, 4.214959649724651, 4.429968988141184, 2.7387716642482376, 2.5620740687262114, 2.6164384660564295, 2.8278977253501063, 3.531551927730451, 4.987466141933972, 2.2013243510734353, 2.2516500548424254, 2.4792959989775487, 3.297281032257926, 5.38306096745109, 1.994252370797725, 2.5445250005284397, 5.034019194063557, 1.8249274671869515, 3.5424451571442663, 4.0597168949982585, 3.7627049519487956, 3.1269163197464858, 8.643379063170546, 0.9953906080094713, 1.1401515805883884, 1.3661702391249748, 1.7852287790094021, 2.925439595995659, 5.647933201038428, 2.8965093607811623, 2.5861443654901297, 3.8607258357338887, 3.239980804453172, 4.801927673477474, 3.008185605705771, 3.4718538723843118, 4.867293117582998, 2.5286829868409795, 3.460174078649966, 4.6609051264192, 2.6059339311451417, 3.509356299665221, 7.29104253218296, 1.1731113016457972, 1.443563820730815, 2.009929107752324, 4.282741809473703, 2.8259520132303475, 9.414519659095346, 5.788058126134665, 3.570076073775372, 2.8205803404161203, 2.6597991504648433, 2.662831201349973, 2.75024985165756, 2.9886428005203363, 3.637854667389364, 4.775174328574728, 3.3000372575053443, 2.6229441283098303, 2.5475427947477467, 2.624926308010195, 2.8706040334341534, 3.662622826324669, 4.921808391572124, 2.305142163194423, 2.3262918659344334, 2.5243462160646564, 3.2113867684218818, 5.3646722320633735, 1.6699809894619686, 2.067964056101407]\n"
     ]
    },
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: plot not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: plot not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] CG(A::Matrix{Complex{Int64}}, b::Vector{Int64}; x0::Nothing, tol::Float64)\n",
      "   @ Main ~/Repos/Advanced-Numerical-Analysis-Course/lecture/lecture_07.ipynb:81\n",
      " [2] top-level scope\n",
      "   @ ~/Repos/Advanced-Numerical-Analysis-Course/lecture/lecture_07.ipynb:3"
     ]
    }
   ],
   "source": [
    "A = [[2, -im] [im, 2]]\n",
    "b = [1; 2]\n",
    "x = CG(A, b, tol=1e-4)\n",
    "y = A \\ b\n",
    "println(\"error: \", norm(x - y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:\n",
      "\n",
      "Stacktrace:\n",
      "  [1] Array\n",
      "    @ ./boot.jl:459 [inlined]\n",
      "  [2] Array\n",
      "    @ ./boot.jl:468 [inlined]\n",
      "  [3] Array\n",
      "    @ ./boot.jl:476 [inlined]\n",
      "  [4] similar\n",
      "    @ ./abstractarray.jl:841 [inlined]\n",
      "  [5] similar\n",
      "    @ ./abstractarray.jl:840 [inlined]\n",
      "  [6] similar\n",
      "    @ ./broadcast.jl:212 [inlined]\n",
      "  [7] similar\n",
      "    @ ./broadcast.jl:211 [inlined]\n",
      "  [8] copy\n",
      "    @ ./broadcast.jl:885 [inlined]\n",
      "  [9] materialize\n",
      "    @ ./broadcast.jl:860 [inlined]\n",
      " [10] broadcast_preserving_zero_d\n",
      "    @ ./broadcast.jl:849 [inlined]\n",
      " [11] +(A::Vector{Float64}, Bs::Vector{Float64})\n",
      "    @ Base ./arraymath.jl:16\n",
      " [12] CG(A::Matrix{Int64}, b::Vector{Int64}, x0::Nothing, tol::Float64)\n",
      "    @ Main ~/Repos/Advanced-Numerical-Analysis-Course/lecture/lecture_07.ipynb:38\n",
      " [13] CG(A::Matrix{Int64}, b::Vector{Int64})\n",
      "    @ Main ~/Repos/Advanced-Numerical-Analysis-Course/lecture/lecture_07.ipynb:6\n",
      " [14] top-level scope\n",
      "    @ ~/Repos/Advanced-Numerical-Analysis-Course/lecture/lecture_07.ipynb:5"
     ]
    }
   ],
   "source": [
    "n = 3;\n",
    "A = collect(reshape(1:(n^2), (n,n)));\n",
    "b = collect(1:n);\n",
    "x0 = zeros(n);\n",
    "x = CG(A, b);\n",
    "\n",
    "print(norm(A * x - b))\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
