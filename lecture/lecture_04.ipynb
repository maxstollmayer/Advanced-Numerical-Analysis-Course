{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4\n",
    "\n",
    "$$\n",
    "\\global\\let\\phi=\\varphi\n",
    "\\global\\let\\eps=\\varepsilon\n",
    "\\gdef\\N{\\mathbb{N}}\n",
    "\\gdef\\F{\\mathbb{F}}\n",
    "\\gdef\\R{\\mathbb{R}}\n",
    "\\gdef\\C{\\mathbb{C}}\n",
    "\\gdef\\L{\\mathcal{L}}\n",
    "\\gdef\\rank{\\mathrm{rank}\\,}\n",
    "\\gdef\\mat{\\mathrm{mat}}\n",
    "\\gdef\\norm#1{\\|#1\\|}\n",
    "\\gdef\\matrix#1{\\begin{pmatrix}#1\\end{pmatrix}}\n",
    "\\gdef\\spec{\\bm{\\lambda}}\n",
    "\\gdef\\diag{\\mathrm{diag}}\n",
    "\\gdef\\offdiag{\\mathrm{offdiag}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now apply these results to analyze the convergence of the Jacobi and Gauß-Seidel iterations. For notational convenience, we denote the iteration matrices as $J = -D^{-1}(L + U)$ and $G = -(D + L)^{-1} U$, where we implicitly assume that $A$ has no zeros on the diagonal, which is equivalent to that $D$ is invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem 4.1\n",
    "\n",
    "Let $n \\in \\N$, $A \\in \\C^{n \\ times n}$ be row-wise or column-wise strictly diagonally dominant. Then $D$ and $D + L$ are both invertible and $\\rho(J) < 1$, $\\rho(G) < 1$.\n",
    "\n",
    "##### Proof\n",
    "\n",
    "Consider $A \\in \\C^{n \\times n}$ row-wise strictly diagnally dominant. Let us, for $\\lambda \\in \\C$, define $B(\\lambda) = \\lambda D + L + U \\in \\C^{n \\times n}$ and $C(\\lambda) = \\lambda (D + L) + U \\in \\C^{n \\times n}$.\n",
    "\n",
    "Let us now consider $\\lambda \\in \\C$ s.t. $|\\lambda| \\ge 1$. Then for every $k \\in \\{1, \\dots, n\\}$ we have\n",
    "$$\\begin{align*}\n",
    "|\\lambda A_{kk}| &= |\\lambda| |A_kk| > |\\lambda| \\sum_{j \\in \\{1, \\dots, k\\} \\setminus \\{k\\}} |A_{kj}| \\\\\n",
    "\\mathrm{(i)} \\quad &\\ge |\\lambda| \\sum_{j=1}^{k-1} |A_{kj}| + \\sum_{j=k+1}^n |A_{kj}| = \\sum_{j=1}^{k-1} |\\lambda A_{kj}| + \\sum_{j=k+1}^n |A_{kj}| \\\\\n",
    "\\mathrm{(ii)} \\quad &\\ge \\sum_{j=1}^{k-1} |A_{kj}| + \\sum_{j=k+1}^n |A_{kj}| \\\\\n",
    "\\end{align*}$$\n",
    "Inequality $\\mathrm{(i)}$ gives $C(\\lambda)_{kk} > \\sum_{j \\in \\{1, \\dots, n\\} \\setminus \\{k\\}} |C(\\lambda)_{kj}|$ and inequality $\\mathrm{(ii)}$ gives $B(\\lambda)_{kk} > \\sum_{j \\in \\{1, \\dots, n\\} \\setminus \\{k\\}} |B(\\lambda)_{kj}|$.\n",
    "So $B(\\lambda)$ and $C(\\lambda)$ are row-wise strictly diagonally dominant.\n",
    "\n",
    "Since $A$ is strictly diagonally dominant, the diagonal entries of $A$ are non-zero, so that $D$ is invertible. $D + L$ is lower-triangular with no zeros on the diagonal, so it is also invertible.\n",
    "\n",
    "For any $\\lambda \\in \\C$ we have\n",
    "$$\\begin{align*}\n",
    "\\det(J - \\lambda I) &= (-1)^n (\\lambda I + D^{-1} (L + U)) \\\\\n",
    "&= (-1)^n \\det(D^{-1}) \\det(\\lambda D + L + U) \\\\\n",
    "&= (-1)^n \\det(D^{-1}) \\det B(\\lambda),\n",
    "\\end{align*}$$\n",
    "so that $\\det(J - \\lambda I) = 0 \\iff \\det B(\\lambda) = 0$.\n",
    "Whenever $|\\lambda| \\ge 1$, $B(\\lambda)$ is non-singular by theorem 3.8, so that $\\lambda \\notin \\spec\\big(B(\\lambda)\\big)$ and hence $\\lambda \\notin \\spec(J).$ This shows that $\\rho(J) <1$.\n",
    "\n",
    "Similarly for any $\\lambda \\in \\C$ we obtain\n",
    "$$\\begin{align*}\n",
    "\\det(G - \\lambda I) &= (-1)^n (\\lambda I + (D+L)^{-1} U) \\\\\n",
    "&= (-1)^n \\det (D + L)^{-1} \\det(\\lambda (D+L) + U) \\\\\n",
    "&= (-1)^n \\det (D + L)^{-1} \\det C(\\lambda),\n",
    "\\end{align*}$$\n",
    "i.e., $\\det(G - \\lambda I) = 0 \\iff \\det C(\\lambda) = 0$.\n",
    "For $|\\lambda| \\ge 1$, $C(\\lambda)$ is non-singular by theorem 3.8, so that $\\lambda \\notin \\spec\\big(C(\\lambda)\\big)$ and hence $\\lambda \\notin \\spec(G)$. This yields $\\rho(G) < 1$.\n",
    "\n",
    "The case of column-wise strict diagonal dominance is treated analogously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corollary 4.2\n",
    "\n",
    "Consider $n \\in \\N$ and $A \\in \\C^{n \\times n}$ satisfying the hypothesis of theorem 4.1 and $b \\in \\C^n$. Then the linear system $A x = b$ has a unique solution $x \\in \\C^n$ and the Jacobi and Gauß-Seidel iterations converge to this exact solution: for any norm $\\norm{\\cdot}$ on $\\C^n$ and for either method, there exists a positive constant $C$ such that\n",
    "$$ \\norm{x_k - x} \\le C (\\rho + \\eps)^k \\norm{x_0 - x}, $$\n",
    "where $\\rho$ is the spectral radius of the respective iteration matrix.\n",
    "\n",
    "##### Proof\n",
    "\n",
    "The claim follows from lemma 3.6 and theorem 4.1 since $x_k - x = J^k (x_0 - x)$ for the Jacobi method and $x_k - x = G^k (x_0 - x)$ for the Gauß-Seidel method for all $k \\in \\N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 4.3\n",
    "\n",
    "For $n \\in \\N$ consider the following Toeplitz tridiagonal matrices:\n",
    "$$ S = \\matrix{2 && -1 && && &&  \\\\ -1 && \\ddots && \\ddots && && \\\\ && \\ddots && \\ddots && \\ddots && \\\\ && && \\ddots && \\ddots && -1 \\\\ && && && -1 && 2} \\in \\R^{n \\times n} \\quad \\quad M = \\matrix{4 && 1 && && &&  \\\\ 1 && \\ddots && \\ddots && && \\\\ && \\ddots && \\ddots && \\ddots && \\\\ && && \\ddots && \\ddots && 1 \\\\ && && && 1 && 4} \\in \\R^{n \\times n} $$\n",
    "\n",
    "For example, matrix $S$ arises as a discretization of the operator of negated second-order derivative by finite-difference and finite-element methods. Matrix $M$ arises as a discretization of the identity operator by a finite-element method. Matrices $S$ and $M$, in that context, are often called **stiffness** and **mass matrices**. Even though they may look similar, they differ dramatically in their properties, ever more as $n$ grows as they approximate better and better the operators $-(\\cdot)''$ and the identity operator.\n",
    "\n",
    "For a Toeplitz tridiagonal matrix\n",
    "$$ T = \\matrix{0 && 1 && && && \\\\ 1 && \\ddots && \\ddots && && \\\\ && \\ddots && \\ddots && \\ddots && \\\\ && && \\ddots && \\ddots && 1 \\\\ && && && 1 && 0} \\in \\R^{n \\times n} $$\n",
    "the eigenvalues can be calculated analytically $\\spec(T) = \\big\\{2 \\cos \\frac{\\pi j}{n+1} \\big\\}_{j=1}^n$.\n",
    "\n",
    "For the two above matrices we therefore have for any $n \\in \\N$\n",
    "$$ \\spec(S) = \\bigg\\{ 4 \\sin \\Big(\\frac{\\pi j}{2(n+1)}\\Big)^2 \\bigg\\}_{j=1}^n \\quad \\quad \\spec(M) = \\bigg\\{ 4 + 2 \\cos \\frac{\\pi j}{n+1} \\bigg\\}_{j=1}^n $$\n",
    "\n",
    "This gives, as $n \\to \\infty$\n",
    "$$ \\spec_{\\min}(S) = 4 \\sin \\Big( \\frac{\\pi}{2(n+1)} \\Big) \\sim \\frac{\\pi^2}{n^2} $$\n",
    "$$ \\spec_{\\max}(S) = 4 \\cos \\Big( \\frac{\\pi}{2(n+1)} \\Big) \\sim 4 $$\n",
    "$$ \\spec_{\\min}(M) = 6 - 4 \\cos \\Big( \\frac{\\pi}{2(n+1)} \\Big) \\sim 2 $$\n",
    "$$ \\spec_{\\max}(M) = 6 - 4 \\sin \\Big( \\frac{\\pi}{2(n+1)} \\Big) \\sim 6 $$\n",
    "\n",
    "$M$ is said to be **spectrally equivalent** to $I$ uniformly w.r.t. $n \\in \\N$ because $c \\norm{x}_2^2 \\le x* M x \\le C \\norm{x}_2^2$ holds with positive $c$ and $C$ independent of $n$.\n",
    "\n",
    "Both $S$ and $M$ are invertible, symmetric and positive-definite. Levy-Desplanque theorem, however, yields invertibility only for $M$ and not for $S$ since the latter is not strictly diagonally dominant. The first Gershgorin theorem yields $\\spec(S) \\subset [0,4]$ and $\\spec(M) \\subset [2,6]$. We see that these bounds are sharp and cannot be improves for all $n \\in \\N$ at once. On the other hand, the first Gershgorin theorem does not tell us that $\\spec_{\\min}(S) > 0$, which is actually the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 4.4\n",
    "\n",
    "Consider $S, M \\in \\R^{n \\times n}$ from example 4.3 with $n \\in \\N$. For $M$, corollary 4.2 shows that the Jacobi and Gauß-Seidel methods applied to the linear system $M x= b$ converge exponentially fast for any initial guess $x_0$. For $S x = b$, our results fail to show convergence.\n",
    "\n",
    "However one can strengthen the analysis given in theorem 4.1 and corollary 4.2 as follows:\n",
    "\n",
    "if $A \\in \\C^{n \\times n}$ has non-zero diagonal entries, is non-strictily diagonally dominant and is irreducible (i.e. $\\Pi A \\Pi^\\top$ is not block upper-triangular for any permutation matrix $\\Pi$ of order $n$) and strictly diagonally dominant in at least one row or column, then $A$ is invertible, $J$ and $G$ are well-defined, $\\rho(J) < 1$, $\\rho(G) < 1$ and te Jacobi and Gauß-Seidel methods converge for any initial guess $x_0 \\in \\C^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Jacobi and Gauß-Seidel methods are particular examples of linear iterative methods, which, at each step $k \\in \\N$ are based on splitting $A = P_k - N_k$ such that $P_k$ is invertible and define $x_k = P_k^{-1} (N_k x_{k-1} + b) = P_k^{-1} N_k x_{k-1} + P_k^{-1} b = B_k x_{k-1} + P_k^{-1} b$ where $B_k = P_k^{-1} N_k = I - P_k^{-1} A$ is the iteration matrix at step $k \\in \\N$.\n",
    "> \n",
    "> Note also that $x_k = P_k^{-1} (P_k x_{k-1} - A x_{k-1} + b) = x_{k-1} + P_k^{-1} r_{k-1}$, where $r_{k-1} = b - A x_{k-1} = A (x - x_{k-1})$ is the **residual vector** corresponding to the $(k-1)$-th iterate.\n",
    "> \n",
    "> As we did abve for the Jacobi and Gauß-Seidel methods, we assume that $x$ is a unique solution of $A x = b$ and set $e_k = x_k - x$ for all $k \\in \\N$ for the iterative process starting at $x_0 \\in F^n$. Then\n",
    "> $$ e_k = x_k - x = x_{k-1} - x + P_k^{-1} A (x - x_{k-1}) = (I - P_k^{-1} A) e_{k-1} = B_k e_{k-1} $$\n",
    "> for each $k \\in \\N$.\n",
    "> \n",
    "> The exponential convergence $x_k \\to x$ as $k \\to \\infty$ can be established, for example, on the basis of $\\norm{B_k} \\le \\rho$ for all $k \\in \\N$ and for some norm $\\norm{\\cdot}$ on $\\F^{n \\times n}$ and $\\rho \\in [0, 1)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
