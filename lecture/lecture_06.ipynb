{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6\n",
    "\n",
    "$$\n",
    "\\global\\let\\phi=\\varphi\n",
    "\\global\\let\\eps=\\varepsilon\n",
    "\\gdef\\N{\\mathbb{N}}\n",
    "\\gdef\\F{\\mathbb{F}}\n",
    "\\gdef\\R{\\mathbb{R}}\n",
    "\\gdef\\C{\\mathbb{C}}\n",
    "\\gdef\\L{\\mathcal{L}}\n",
    "\\gdef\\K{\\mathcal{K}}\n",
    "\\gdef\\P{\\mathcal{P}}\n",
    "\\gdef\\Q{\\mathcal{Q}}\n",
    "\\gdef\\rank{\\mathrm{rank}\\,}\n",
    "\\gdef\\mat{\\mathrm{mat}}\n",
    "\\gdef\\abs#1{\\left|#1\\right|}\n",
    "\\gdef\\norm#1{\\left\\|#1\\right\\|}\n",
    "\\gdef\\matrix#1{\\begin{pmatrix}#1\\end{pmatrix}}\n",
    "\\gdef\\spec{\\bm{\\lambda}}\n",
    "\\gdef\\diag{\\mathrm{diag}}\n",
    "\\gdef\\span#1{\\mathrm{span}\\left\\{#1\\right\\}}\n",
    "\\gdef\\offdiag{\\mathrm{offdiag}}\n",
    "\\gdef\\O{\\mathcal{O}}\n",
    "\\gdef\\nto{\\nrightarrow}\n",
    "\\gdef\\Re{\\mathrm{Re}}\n",
    "\\gdef\\Im{\\mathrm{Im}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the last lecute, we observerd that a gradient descent iteration for the minimization of a quadratic functional coincides with the Richardson iteration for the corresponding linear system expressing the first-roder optimality condition (the two methods also share a sequence of relaxation parameters). Further, the accuracy of approximate solutions may have to be measured in the norm induced by the SPD matrix of the system.\n",
    "\n",
    "We will now prove an auxiliary result useful for working with such induced norms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemma 6.1\n",
    "\n",
    "Let $n \\in \\N$ and $A \\in \\R^{n \\times n}$ be a symmetric positive-definite matrix. Then there exists a unique symmetric positive-definite matrix $B \\in \\R^{n \\times n}$ such that $B^2 = A$.\n",
    "\n",
    "##### Proof\n",
    "\n",
    "As an SPD matrix, $A$ has a spectral decomposition $A = Q \\Lambda Q^\\top$, where $Q \\in \\R^{n \\times n}$ is orhtogonal and $\\Lambda \\in \\R^{n \\times n}$ is diagonal with positive diagonal entries. Withoug loss of generality, we may assume that $\\Lambda = \\diag(\\lambda_1 I_{s_1}, \\dots, \\lambda_r I_{s_r})$, where $\\lambda_1, \\dots, \\lambda_r > 0$ with $r \\in \\{1, \\dots, n\\}$ are the distinct eigenvalues of $A$ and $s_1, \\dots, s_r \\in \\{1, \\dots, n\\}$ are their multiplicities satisfying $s_1 + \\dots + s_r = n$ (otherwise the terms in the spectral decomposition of $A$ can be reordered).\n",
    "\n",
    "For $B = Q \\sqrt{\\Lambda} Q^\\top$, where $\\sqrt{\\Lambda} = \\diag(\\sqrt{\\lambda_1} I_{s_1}, \\dots, \\sqrt{\\lambda_r} I_{s_r})$, we have $B^2 = Q \\Lambda Q^\\top = A$. Since $B$ is SPD, this proves the existence part of the claim.\n",
    "\n",
    "Let $\\tilde{B} \\in \\R^{n \\times n}$ be an SPD matrix such that $\\tilde{B}^2 = A$. As an SPD matrix, $\\tilde{B}$ has a spectral decomposition $\\tilde{B} = \\tilde{Q} D \\tilde{Q}^\\top$ with $\\tilde{Q} \\in \\R^{n \\times n}$ orthogonal and $D \\in \\R^{n \\times n}$ diagonal with positive diagonal entries. Then $\\tilde{B}^2 = A$ yields $\\tilde{Q} D^2 \\tilde{Q}^\\top = A$, i.e., $D^2$ is similar to $A$ and hence to $\\Lambda$. $D^2$ and $\\Lambda$ are diagonal matrices, so their similarity means that the diagonal matrices, so their similarity means that the diagonal of $D^2$ is a permutation of that of $\\Lambda$.\n",
    "\n",
    "W.l.o.g. we may assume that $D^2 = \\Lambda$ (otherwise the terms in the spectral decomposition of $\\tilde{B}$ can be reordered). So we have two spectral decompositions for $A$: $A = Q \\Lambda Q^\\top = \\tilde{Q} \\Lambda \\tilde{Q}^\\top$.\n",
    "\n",
    "Let us partition $Q$ and $\\tilde{Q}$ as follows: $Q = \\matrix{Q_1 && \\dots && Q_r}$ and $\\tilde{Q} = \\matrix{\\tilde{Q}_1 && \\dots && \\tilde{Q}_r}$ with $Q_k, \\tilde{Q}_k \\in \\R^{n \\times s_k}$ for all $k \\in \\{1, \\dots, r\\}$. Then, for each $k \\in \\{1, \\dots, r\\}$, the columns of $Q_k$ and the columns of $\\tilde{Q}_k$ form two orhtonormal bases for the same subspace, namely the eigenspace of $A$ corresponding to eigenvalue $\\lambda_k$, so that there exists $V_k \\in \\R^{s_k \\times s_k}$ such that $\\tilde{Q}_k = Q_k V_k$. Then $\\tilde{B} = \\tilde{Q} \\ \\sqrt{\\Lambda} \\ \\tilde{Q}^\\top = Q \\ \\diag(V_1, \\dots, V_r) \\ \\diag(\\sqrt{\\lambda_1} I_{s_1}, \\dots, \\sqrt{\\lambda_r} I_{s_r}) \\ \\diag(V_1^\\top, \\dots, V_r^\\top) \\ Q^\\top = Q \\ \\sqrt{\\Lambda} \\ Q^\\top = B$, which proves the uniqueness claim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The SPD matrix $B$ prves to exist for any SPD matrix $A$ in lemma 6.1 is called the **principal square root** or the **SPD square root** of $A$.\n",
    "> \n",
    "> Using lemma 6.1 we can derive the following convergence bounds for the Richardson iteration (gradient descent) applied to an SPD matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem 6.2\n",
    "\n",
    "Let $A, M \\in \\R^{n \\times n}$ be commuting SPD matrices. Then the Richardson iteration for $A x = b$ with any $b \\in \\R^n$ satisfies\n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\norm{e_k}_M \\le \\norm{I - \\alpha_k A}_2 \\ \\norm{e_{k-1}}_M, \\\\\n",
    "&\\norm{e_k}_M \\le \\norm{(I - \\alpha_k A) \\dots (I - \\alpha_1 A)}_2 \\ \\norm{e_0}_M\n",
    "\\end{align*}$$\n",
    "\n",
    "##### Proof\n",
    "\n",
    "let $\\sqrt{M}$ be the SPD square root of $M$. Then\n",
    "\n",
    "$$ \\norm{e_k}_M = \\norm{\\sqrt{M} e_k}_2 = \\norm{\\sqrt{M} (I - \\alpha_k A) e_{k-1}}_2 = \\norm{(I - \\alpha_k A) \\sqrt{M} e_{k-1}}_2 $$\n",
    "\n",
    "This gives, first,\n",
    "\n",
    "$$ \\norm{e_k}_M \\le \\norm{I - \\alpha_k A}_2 \\ \\norm{\\sqrt{M} e_{k-1}}_2 = \\norm{I - \\alpha_k A}_2 \\ \\norm{e_{k-1}}_A $$\n",
    "\n",
    "and, second,\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\norm{e_k}_M &= \\norm{(I - \\alpha_k A) (I - \\alpha_{k-1} A) \\sqrt{M} e_{k-2}}_2 \\\\\n",
    "&= \\dots = \\norm{(I - \\alpha_k A) \\dots (I - \\alpha_1 A) \\sqrt{M} e_0}_2 \\\\\n",
    "&\\le \\norm{(I - \\alpha_k A) \\dots (I - \\alpha_1 A)}_2 \\ \\norm{\\sqrt{M} e_0}_2 \\\\\n",
    "&= \\norm{(I - \\alpha_k A) \\dots (I - \\alpha_1 A)}_2 \\ \\norm{e_0}_M\n",
    "\\end{align*}$$\n",
    "\n",
    "for each $k \\in \\N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In particular, theorem 6.2 holds for $M = I$, in which case the proof does not involve SPD square roots.\n",
    "> \n",
    "> We will now consider a generalization of the Richardson iteration. To this end, let us consider the Richardson iteration $x_k = x_{k-1} + \\alpha_k r_{k-1}$ for $k \\in \\N$ and $A x = b$ starting at $x_0$, where $r_k = b - A x_k$ and $\\alpha_k$ are relaxation parameters.\n",
    ">\n",
    "> At this point, we are interested in several observations. First, for notational convenience, we set $\\K_k = \\span{A^j r_0}_{j=0}^{k-1}$ for all $k \\in \\N$. Clearly we have $\\K_k \\subseteq \\K_{k+1}$. With this notation we note the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 6.3\n",
    "\n",
    "$r_{k-1} \\in \\K_k$ and $x_k - x_0 \\in \\K_k$ for all $k \\in \\N$. This is easy to show by induction: first $r_0 \\in \\span{r_0} = \\K_1$ holds. Further, for any $k \\in \\N$, assuming $r_{k-1} \\in \\K_k$, we obtain $r_k = r_{k-1} - A (x_k - x_{k-1}) = r_{k-1} - A r_{k-1} \\in \\K_{k+1}$ so that $r_{k-1} \\in \\K_k$. Then $x_k - x_0 = \\sum_{j=1}^k \\alpha_j r_{j-1} \\in \\K_k$ for all $k \\in \\N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 6.4\n",
    "\n",
    "We can interpret remark 6.3 as follows:\n",
    "\n",
    "For each $k \\in \\N$ the $k$-th **search direction**, in which we seek an update of the current iterate is the corresponding residual $x_{k-1} - x_0 \\in \\K_{k-1}$ and $x_k - x_{k-1} \\in \\span{r_{k-1}}$, so $r_{k-1}$ is the new dimension with which the $(k-1)$-th solution space $\\K_{k-1}$ is expanded to the $k$-th solution space $\\K_k$: $\\K_k = \\K_{k-1} + \\span{r_{k-1}}$ for all $k \\in \\N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 6.5\n",
    "\n",
    "The errors $e_k = x_k - x$ for $k \\in \\N_0$ satisfy\n",
    "\n",
    "$$ e_k = (I - \\alpha_k A) \\dots (I - \\alpha_1 A) e_0 = q_k(A) e_0, $$\n",
    "\n",
    "where $q_k \\in \\P_k$ is an algebraic polynomial of degree $k$, given by $q_k(t) = \\prod_{j=1}^k (1 - \\alpha_j t)$ for all $t \\in \\R$ and $q_k(A) = \\prod_{j=1}^k (I - \\alpha_j A) \\in \\R^{n \\times n}$ (the factors commute, so this definition of $q_k(A)$ is correct).\n",
    "\n",
    "The above definition implies $q_0 = 1$. For each $k \\in \\N$ we have $q_k(t) = (1 - \\alpha_k t) q_{k-1}(t)$. The form of the first factor is due to that $e_k = (I - \\alpha_k A) e_{k-1}$, which is a consequence of $x_k = 1 x_{k-1} + \\alpha_k r_{k-1}$. So the iteration implies $q_k(0) = 1$ for all $k \\in \\N$. So $q_k \\in \\Q_k$ for $k \\in \\N_0$, where $\\Q_k = \\{ q \\in \\P_k \\ : \\ q(0) = 1, \\deg q = k \\} \\subseteq \\P_k$.\n",
    "\n",
    "On the other hand, for all $k \\in \\N$ and for all $\\tilde{q}_k \\in \\Q_k$ zero s not a root of $\\tilde{q}_k$, so that $\\tilde{q}_k(t) = \\prod_{j=1}^k (1 - \\tilde{\\alpha}_j t)$ for all $t \\in \\R$ with some $\\tilde{\\alpha}_1, \\dots, \\tilde{\\alpha}_k \\in \\C \\setminus \\{0\\}$.\n",
    "\n",
    "This means that the Richardson iteration, which we originally parametrized by $\\alpha_k \\in \\R \\setminus \\{0\\}$ with $k \\in \\N$ can be equivalently parametrized by $q_k \\in \\Q_k$ with\n",
    "\n",
    "- all roots reald and such that\n",
    "- for each $k \\in \\N$ the roots of $q_k$ are also roots of $q_{k-1}$ with the same or larger multiplicities (this restriction is equivalent to $q_{k+1}(t) = (1 - \\alpha_{k+1} t) q_k(t)$ for all $t \\in \\R$ with $\\alpha_{k+1} \\in \\R \\setminus \\{0\\}$).\n",
    "\n",
    "These two restrictions we shall lift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark 6.6\n",
    "\n",
    "Finally, $e_k = q_k(A) e_0$ for all $k \\in \\N$ implies $x_k - x_0 = e_k - e_0 = \\big(I - q_k(A)\\big) e_0 = \\big(I - q_k(A)\\big) A^{-1} r_0 = \\pi_k(A) r_0$ for all $k \\in \\N_0$, where $\\pi_0 = 0 \\in \\P_0$ and for each $k \\in \\N$, $\\pi_k \\in \\P_{k-1}$ is given by $\\pi_k(t) = \\frac{1}{t} \\big(1 - q_k(t)\\big)$ for all $t \\in \\R$. $q_k(0) = 1$ ensures that $\\pi_k \\in \\P_{k-1}$.\n",
    "\n",
    "The polynomials $\\pi_k$ are constrained by the choice of $q_k$. By lifting the constraints on $q_k$ discussed in remark 6.5 we will allow for $\\pi_k \\in \\P_{k-1}$ arbitrary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Before generalizing the Richardson iteration, let us note that, for any $M \\in \\R^{n \\times n}$ SPD and commuting with $A$, we have, by theorem 6.2, $\\norm{e_k}_M \\le \\norm{q_k(A)}_2 \\ \\norm{e_0}_M$ for all $k \\in \\N$. Since matrix $A$ is SPD, it has a spectral decomposition $A = Q \\Lambda Q^\\top$ with $Q \\in \\R^{n \\times n}$ orthogonal and $\\Lambda \\in \\R^{n \\times n}$ diagonal with positive diagonal entries. Then $q_k(A) = Q \\ q_k(\\Lambda) \\ Q^\\top$ and $\\norm{q_k(A)}_2 = \\norm{q_k(\\Lambda)}_2 = \\max_{t \\in \\spec(A)} \\abs{q_k(t)} \\ \\norm{e_0}_M$ for all $k \\in \\N$.\n",
    "> \n",
    "> We will design methods, other than the Richardson method, that minimize $\\norm{e_k}_M$ for each $k \\in \\N$. The Richardson iteration will, however, be very useful in the analysis of such methods: any methods minimizing $\\norm{e_k}_M$ should perform not worse than the first $k$ Richardson iteration with any suitable $\\alpha_1, \\dots, \\alpha_k$, and we will consider $\\alpha_1, \\dots, \\alpha_k$ that result in provably small values of $\\max_{t \\in \\spec(A)} \\abs{q_k(t)}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
